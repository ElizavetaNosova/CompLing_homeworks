{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Topicmodeling_gensim.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPw7wgUewZac5BZBnpcH8z0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElizavetaNosova/CompLing_homeworks/blob/master/Topicmodeling_gensim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4VgExSg79rs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "outputId": "999bc946-8514-41fc-bc3c-0c0b38a8c6b5"
      },
      "source": [
        "!pip install gensim\n",
        "!pip install pandas\n",
        "!pip install nltk\n",
        "!pip install pymorphy2[fast]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.17.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.9.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.11.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.21.0)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.9 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.14.9)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.3.2)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2019.11.28)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.9->boto3->smart-open>=1.2.1->gensim) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.9->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.25.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.17.5)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Collecting pymorphy2[fast]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 1.6MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2[fast]) (0.6.2)\n",
            "Collecting pymorphy2-dicts<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 6.3MB/s \n",
            "\u001b[?25hCollecting DAWG>=0.7.3; extra == \"fast\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/c0/d8d967bcaa0b572f9dc1d878bbf5a7bfd5afa2102a5ae426731f6ce3bc26/DAWG-0.7.8.tar.gz (255kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 38.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: DAWG\n",
            "  Building wheel for DAWG (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for DAWG: filename=DAWG-0.7.8-cp36-cp36m-linux_x86_64.whl size=771493 sha256=0ad8b821482700ae370c9fa9469f715775a7957ec7b39e66848628e310a935d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/88/d0/4e4abc83eb8f59a71e8dbd8ba99fd5615a3af1fac1ef7f8825\n",
            "Successfully built DAWG\n",
            "Installing collected packages: dawg-python, pymorphy2-dicts, DAWG, pymorphy2\n",
            "Successfully installed DAWG-0.7.8 dawg-python-0.7.2 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKf5HkSQ_dQu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "outputId": "6af05aa0-4a76-486b-8c53-3aff48be24ae"
      },
      "source": [
        "!pip install pyLDAvis"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyLDAvis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.34.2)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.17.5)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.25.3)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.14.1)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.11.1)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.7.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (3.6.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Collecting funcy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/4b/6ffa76544e46614123de31574ad95758c421aae391a1764921b8a81e1eae/funcy-1.14.tar.gz (548kB)\n",
            "\u001b[K     |████████████████████████████████| 552kB 39.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.3.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (8.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.12.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (19.3.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.8.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (0.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (45.1.0)\n",
            "Building wheels for collected packages: pyLDAvis, funcy\n",
            "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97711 sha256=a9c697fe3c03370cf836c26327d7d2a170e7640fb1758e03ffea8d66f7e9fefa\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n",
            "  Building wheel for funcy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for funcy: filename=funcy-1.14-py2.py3-none-any.whl size=32042 sha256=b223e7455751226f1c1432dbef75f504d8f1217e5f523a16fdf913bb1e097936\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/5a/d8/1d875df03deae6f178dfdf70238cca33f948ef8a6f5209f2eb\n",
            "Successfully built pyLDAvis funcy\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-1.14 pyLDAvis-2.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjmNkeOD8IJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "import pyLDAvis.gensim\n",
        "import string\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "morph = MorphAnalyzer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl_2dS8L_w8x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "932aa15a-16d8-4607-8be1-144afdbe8dff"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPBO7NV_BGi8",
        "colab_type": "text"
      },
      "source": [
        "При нормализации сразу не будем включать слова из списка стоп-слов и другие слова служебных частей речи, местоимения (видимо, местоимения-существительные, так как pymorphy только их выделяет отдельно) и числительные.\n",
        "Полагаю, ято в связи с этим список стопслов можно изначально урезать: скопировать из nltk и оставить только другие части речи, раз вышеупомянутые всё равно будут проверяться отдельно.\n",
        "Впрочем, непонятно, зачем я это делаю, если эти слова всё равно вряд ли станут \"ключевыми\" для тем"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNeb1bt8BFkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stops = ['как','так', 'было', 'еще', 'нет', 'теперь', 'когда', 'вдруг', 'уже', 'быть', 'был', 'нибудь', 'опять', 'там', 'потом', 'себя', 'ничего', 'может',  'тут', 'где', 'есть', 'надо', 'чем', 'была', 'сам', 'раз', 'тоже', 'себе', 'под', 'будет', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве',  'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n",
        "\n",
        "def remove_tags(text):\n",
        "    return re.sub(r'<[^>]+>', '', text)\n",
        "\n",
        "def opt_normalize(texts, n):\n",
        "    uniq = Counter()\n",
        "    for text in texts:\n",
        "        uniq.update(text)\n",
        "    \n",
        "    norm_uniq = {word:morph.parse(word)[0].normal_form for word, _ in uniq.most_common(n) if morph.parse(word)[0].tag.POS not in ['NUMR', 'NPRO', 'PREP', 'CONJ', 'PRCL', 'INTJ']}\n",
        "    \n",
        "    norm_texts = []\n",
        "    for text in texts:\n",
        "        \n",
        "        norm_words = [norm_uniq.get(word) for word in text]\n",
        "        norm_words = [word for word in norm_words if word and word not in stops]\n",
        "        norm_texts.append(norm_words)\n",
        "        \n",
        "    return norm_texts\n",
        "\n",
        "def tokenize(text):\n",
        "    words = [word.strip(string.punctuation) for word in text.split()]\n",
        "    words = [word for word in words if word]\n",
        "    \n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgNGWUxQJhLM",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "847279ca-77b5-41db-88da-88ded31cf0f7"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9e49bbfb-2e15-4220-a97e-b18223ca9193\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-9e49bbfb-2e15-4220-a97e-b18223ca9193\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving habr_texts.txt to habr_texts.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nvNzwT1MtPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts = open('habr_texts.txt').read().splitlines()\n",
        "texts = opt_normalize([tokenize(remove_tags(text.lower())) for text in texts], 30000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwvgiaHAbU04",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "919075d1-64c4-4e4e-e25e-de8668bbad7b"
      },
      "source": [
        "print(len(texts))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4121\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJVZcTfQOHCG",
        "colab_type": "text"
      },
      "source": [
        "Подберём параметры, при которых когерентность будет наибольшей. Для начала поберём параметры нграм и словаря при каких-нибудь параметрах модели. Будем надеяться, что при других параметрах модели они тоже хорошие"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYCDZR17N28x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 934
        },
        "outputId": "48c66efc-9b41-4e0f-a059-1389097bcfa4"
      },
      "source": [
        "ngram_threshold_values = [0.2, 0.4, 0.6, 0.8]\n",
        "no_above_values = [0, 0.05, 0.1, 0.2, 0.25]\n",
        "no_below_values = [10, 20, 30, 50]\n",
        "best_res = 0\n",
        "best_params = {}\n",
        "for ngram_threshold in ngram_threshold_values:\n",
        "  for no_above in no_above_values:\n",
        "    for no_below in no_below_values:\n",
        "      try:\n",
        "        ph = gensim.models.Phrases(texts, scoring='npmi', threshold=ngram_threshold) # threshold можно подбирать\n",
        "        p = gensim.models.phrases.Phraser(ph)\n",
        "        ngrammed_texts = p[texts]\n",
        "        dictinary = gensim.corpora.Dictionary(texts)\n",
        "        dictinary.filter_extremes(no_above=no_above, no_below=no_below)\n",
        "        dictinary.compactify()\n",
        "        corpus = [dictinary.doc2bow(text) for text in texts]\n",
        "        lda = gensim.models.LdaMulticore(corpus, 100, id2word=dictinary, eval_every=0)\n",
        "        coherence_model_lda = gensim.models.CoherenceModel(model=lda, \n",
        "                                                    texts=texts, \n",
        "                                                     dictionary=dictinary, coherence='c_v')\n",
        "        if coherence_model_lda.get_coherence() > best_res:\n",
        "          best_res = coherence_model_lda.get_coherence()\n",
        "          best_params['ngram_threshold'] = ngram_threshold\n",
        "          best_params['no_above'] = no_above\n",
        "          best_params['no_below'] = no_below\n",
        "      except Exception as e:\n",
        "         print(e)\n",
        "print(best_params)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cannot compute LDA over an empty collection (no terms)\n",
            "cannot compute LDA over an empty collection (no terms)\n",
            "cannot compute LDA over an empty collection (no terms)\n",
            "cannot compute LDA over an empty collection (no terms)\n",
            "cannot compute LDA over an empty collection (no terms)\n",
            "cannot compute LDA over an empty collection (no terms)\n",
            "cannot compute LDA over an empty collection (no terms)\n",
            "cannot compute LDA over an empty collection (no terms)\n",
            "cannot compute LDA over an empty collection (no terms)\n",
            "cannot compute LDA over an empty collection (no terms)\n",
            "cannot compute LDA over an empty collection (no terms)\n",
            "cannot compute LDA over an empty collection (no terms)\n",
            "cannot compute LDA over an empty collection (no terms)\n",
            "cannot compute LDA over an empty collection (no terms)\n",
            "cannot compute LDA over an empty collection (no terms)\n",
            "cannot compute LDA over an empty collection (no terms)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process ForkPoolWorker-56:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 103, in worker\n",
            "    initializer(*initargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/gensim/models/ldamulticore.py\", line 333, in worker_e_step\n",
            "    worker_lda.do_estep(chunk)  # TODO: auto-tune alpha?\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py\", line 725, in do_estep\n",
            "    gamma, sstats = self.inference(chunk, collect_sstats=True)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py\", line 679, in inference\n",
            "    phinorm = np.dot(expElogthetad, expElogbetad) + eps\n",
            "  File \"<__array_function__ internals>\", line 6, in dot\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-3620d71f2f70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mdictinary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompactify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdictinary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLdaMulticore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         coherence_model_lda = gensim.models.CoherenceModel(model=lda, \n\u001b[1;32m     19\u001b[0m                                                     \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, workers, chunksize, passes, batch, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, random_state, minimum_probability, minimum_phi_value, per_word_topics, dtype)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mgamma_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminimum_probability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminimum_probability\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mminimum_phi_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminimum_phi_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_word_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mper_word_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         )\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;31m# wait for all outstanding jobs to finish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mqueue_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                 \u001b[0mprocess_result_queue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreallen\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlencorpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36mprocess_result_queue\u001b[0;34m(force)\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \"\"\"\n\u001b[1;32m    267\u001b[0m                 \u001b[0mmerged_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m                     \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                     \u001b[0mqueue_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mempty\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSAV8LuV8Sn5",
        "colab_type": "text"
      },
      "source": [
        "Если честно, все варианты я не перебрала, но через какое-то время (часа полтора) лучшим результатом было {'ngram_threshold': 0.8, 'no_above': 0.05, 'no_below': 10}, когерентность 0.3816700738434562.\n",
        "Как минимум все предложенные характеристики нграм хотя бы по разу были опробованы, раз дело дошло до 0.8\n",
        " Промежуточные выводы: \n",
        "1. некоторые параметры отсекают весь корпус (отлавливалась соответствующая ошибка)\n",
        "2. не надо перебирать 80 сочетаний характеристик, если я хочу, чтобы процесс закончился через адекватное время"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1BzYV-l8-j8",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrKQcWXl8JaE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9afe05a5-93d5-492c-e19c-54f3db022e69"
      },
      "source": [
        "print(best_params) \n",
        "print(best_res)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'ngram_threshold': 0.8, 'no_above': 0.05, 'no_below': 10}\n",
            "0.3816700738434562\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mREoQ3m9Agp",
        "colab_type": "text"
      },
      "source": [
        "С учётом второго вывода  подберём настройки модели. Очень хочется верить, что, если корпус будет оформлен один раз, процесс будет хоть немного быстрее."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ri848fkQ91ef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ph = gensim.models.Phrases(texts, scoring='npmi', threshold=0.8) \n",
        "p = gensim.models.phrases.Phraser(ph)\n",
        "nrammed_texts = p[texts]\n",
        "dictinary = gensim.corpora.Dictionary(texts)\n",
        "dictinary.filter_extremes(no_above=0.05, no_below=10)\n",
        "dictinary.compactify()\n",
        "corpus = [dictinary.doc2bow(text) for text in texts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwgU83Ca-g1n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_topics = [50, 100, 200]\n",
        "n_eval = [0,1]\n",
        "best_res = 0\n",
        "best_params = {}\n",
        "best_topics = ''\n",
        "for n_topics in num_topics:\n",
        "  for n in n_eval:\n",
        "      lda = gensim.models.LdaMulticore(corpus, n_topics, id2word=dictinary, eval_every=n)\n",
        "      coherence_model_lda = gensim.models.CoherenceModel(model=lda, \n",
        "                                                texts=texts, \n",
        "                                                  dictionary=dictinary, coherence='c_v')\n",
        "      if coherence_model_lda.get_coherence() > best_res:\n",
        "        best_res = coherence_model_lda.get_coherence()\n",
        "        best_params['topics'] = n_topics\n",
        "        best_params['eval'] = n\n",
        "        best_model = lda"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpXL9RWKNNAa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "outputId": "91259202-b800-4b2f-ca68-3e2d96f8c337"
      },
      "source": [
        "best_model.print_topics()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(34,\n",
              "  '0.019*\"инцидент\" + 0.013*\"фоновый\" + 0.011*\"client\" + 0.010*\"chrome\" + 0.009*\"вкладка\" + 0.009*\"учётный\" + 0.008*\"бюджет\" + 0.007*\"иб\" + 0.007*\"долг\" + 0.006*\"select\"'),\n",
              " (80,\n",
              "  '0.034*\"twitter\" + 0.019*\"твит\" + 0.018*\"бот\" + 0.012*\"аккаунт\" + 0.008*\"выборка\" + 0.008*\"ботнет\" + 0.008*\"тестировщик\" + 0.007*\"общественный\" + 0.006*\"северный\" + 0.006*\"streaming\"'),\n",
              " (38,\n",
              "  '0.036*\"боль\" + 0.032*\"китай\" + 0.015*\"правительство\" + 0.015*\"атрибут\" + 0.015*\"vpn\" + 0.009*\"сопротивление\" + 0.008*\"•\" + 0.007*\"lt;/summary&gt\" + 0.007*\"чиновник\" + 0.007*\"англ\"'),\n",
              " (66,\n",
              "  '0.014*\"клетка\" + 0.009*\"атрибут\" + 0.007*\"математика\" + 0.007*\"женщина\" + 0.006*\"аудио\" + 0.006*\"образование\" + 0.006*\"ген\" + 0.005*\"хакер\" + 0.005*\"ит\" + 0.005*\"семья\"'),\n",
              " (98,\n",
              "  '0.026*\"foreach\" + 0.019*\"итерация\" + 0.018*\"do\" + 0.018*\"apply\" + 0.014*\"вектор\" + 0.013*\"fun\" + 0.009*\"матрица\" + 0.009*\"repeat\" + 0.008*\"result\" + 0.008*\"margin\"'),\n",
              " (61,\n",
              "  '0.040*\"домен\" + 0.012*\"переадресация\" + 0.012*\"миграция\" + 0.008*\"seo\" + 0.008*\"search\" + 0.007*\"console\" + 0.007*\"видимость\" + 0.006*\"перемещение\" + 0.006*\"бренд\" + 0.005*\"риска\"'),\n",
              " (40,\n",
              "  '0.020*\"up\" + 0.009*\"messages\" + 0.008*\"временный\" + 0.007*\"cookie\" + 0.007*\"php\" + 0.006*\"логин\" + 0.006*\"days\" + 0.005*\"use\" + 0.005*\"lt;div\" + 0.005*\"param\"'),\n",
              " (42,\n",
              "  '0.064*\"result\" + 0.026*\"let\" + 0.013*\"yahoo\" + 0.010*\"amount\" + 0.010*\"customer\" + 0.009*\"record\" + 0.008*\"case\" + 0.008*\"1.5\" + 0.007*\"amp;&amp\" + 0.007*\"get\"'),\n",
              " (51,\n",
              "  '0.014*\"клетка\" + 0.012*\"временный\" + 0.007*\"ген\" + 0.006*\"php\" + 0.005*\"виртуализация\" + 0.005*\"broadcast\" + 0.005*\"прерывание\" + 0.005*\"малый\" + 0.005*\"образование\" + 0.004*\"параллельный\"'),\n",
              " (24,\n",
              "  '0.013*\"атрибут\" + 0.011*\"прибор\" + 0.010*\"изделие\" + 0.009*\"стек\" + 0.008*\"статический\" + 0.007*\"площадь\" + 0.007*\"xiaomi\" + 0.006*\"mi\" + 0.005*\"линейка\" + 0.005*\"иконка\"'),\n",
              " (4,\n",
              "  '0.014*\"корея\" + 0.013*\"нативный\" + 0.012*\"кнут\" + 0.008*\"token\" + 0.006*\"южный\" + 0.006*\"веб\" + 0.006*\"бот\" + 0.005*\"инвестиция\" + 0.005*\"коммуникация\" + 0.005*\"lt;&lt\"'),\n",
              " (64,\n",
              "  '0.012*\"кэш\" + 0.011*\"at\" + 0.011*\"iaas\" + 0.011*\"client\" + 0.010*\"paas\" + 0.010*\"arm\" + 0.008*\"сброс\" + 0.008*\"select\" + 0.007*\"where\" + 0.007*\"state\"'),\n",
              " (1,\n",
              "  '0.020*\"яркость\" + 0.014*\"стикер\" + 0.014*\"репликация\" + 0.007*\"гаджет\" + 0.006*\"no\" + 0.006*\"more\" + 0.005*\"атрибут\" + 0.005*\"координата\" + 0.005*\"выделение\" + 0.005*\"цветок\"'),\n",
              " (70,\n",
              "  '0.012*\"админ\" + 0.011*\"домен\" + 0.009*\"users\" + 0.008*\"кожа\" + 0.008*\"state\" + 0.008*\"admin\" + 0.007*\"•\" + 0.007*\"off\" + 0.006*\"net\" + 0.006*\"add\"'),\n",
              " (71,\n",
              "  '0.017*\"стикер\" + 0.008*\"шифрование\" + 0.008*\"зашифровать\" + 0.008*\"выделение\" + 0.007*\"azure\" + 0.006*\"спецификация\" + 0.006*\"доска\" + 0.005*\"mi\" + 0.005*\"vpn\" + 0.005*\"uber\"'),\n",
              " (26,\n",
              "  '0.019*\"while\" + 0.015*\"iaas\" + 0.014*\"spi\" + 0.013*\"paas\" + 0.010*\"0x00\" + 0.007*\"байт\" + 0.006*\"define\" + 0.006*\"поезд\" + 0.005*\"робот\" + 0.005*\"регистр\"'),\n",
              " (33,\n",
              "  '0.007*\"студент\" + 0.004*\"redmine\" + 0.004*\"const\" + 0.004*\"вкладка\" + 0.004*\"trello\" + 0.004*\"key\" + 0.003*\"jira\" + 0.003*\"sum\" + 0.003*\"подсеть\" + 0.003*\"андрей\"'),\n",
              " (74,\n",
              "  '0.013*\"кампания\" + 0.009*\"бот\" + 0.008*\"adwords\" + 0.006*\"объявление\" + 0.006*\"спецификация\" + 0.006*\"скачивание\" + 0.004*\"предпочитать\" + 0.004*\"архив\" + 0.004*\"dvd\" + 0.004*\"dc\"'),\n",
              " (68,\n",
              "  '0.010*\"скидка\" + 0.008*\"токен\" + 0.006*\"char\" + 0.004*\"шлюз\" + 0.004*\"директива\" + 0.003*\"unsigned\" + 0.003*\"iot\" + 0.003*\"акция\" + 0.003*\"ssd\" + 0.003*\"хостинг\"'),\n",
              " (81,\n",
              "  '0.019*\"контейнер\" + 0.008*\"атрибут\" + 0.007*\"docker\" + 0.006*\"авторизация\" + 0.006*\"j\" + 0.006*\"run\" + 0.006*\"input\" + 0.005*\"rm\" + 0.005*\"messages\" + 0.005*\"your\"')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG-00QhrYR_d",
        "colab_type": "text"
      },
      "source": [
        "Лучшие темы:\n",
        "- ситуация в Китае (даже если убрать Китай, сочетание правительства и боли прекрасно)\n",
        "(38,\n",
        "  '0.036*\"боль\" + 0.032*\"китай\" + 0.015*\"правительство\" + 0.015*\"атрибут\" + 0.015*\"vpn\" + 0.009*\"сопротивление\" + 0.008*\"•\" + 0.007*\"lt;/summary&gt\" + 0.007*\"чиновник\" + 0.007*\"англ\"'),\n",
        "\n",
        "- экраны и графика:\n",
        " '0.020*\"яркость\" + 0.014*\"стикер\" + 0.014*\"репликация\" + 0.007*\"гаджет\" + 0.006*\"no\" + 0.006*\"more\" + 0.005*\"атрибут\" + 0.005*\"координата\" + 0.005*\"выделение\" + 0.005*\"цветок\"'),\n",
        "\n",
        " - биоинформатика\n",
        " 51,\n",
        "  '0.014*\"клетка\" + 0.012*\"временный\" + 0.007*\"ген\" + 0.006*\"php\" + 0.005*\"виртуализация\" + 0.005*\"broadcast\" + 0.005*\"прерывание\" + 0.005*\"малый\" + 0.005*\"образование\" + 0.004*\"параллельный\"'),\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7UVtQtEZQMb",
        "colab_type": "text"
      },
      "source": [
        "В выдачу попадало много слов типа \"user\" и команд языков программирования. В принципе, это не помешало, но перед следующим этапом лучше эти слова исключить. Добавим в предобработку ограничение: будем учитывать только слова, написанные кириллицей.\n",
        "Ещё попало имя \"Андрей\". В идеале стоило бы скопировать словарь имён и стереть оттуда имена известных чат-ботов и т.п., но будем надеяться, что корреляция имён с темами маловероятна."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9_VdZ5ca0-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_english(text):\n",
        "  new_text = re.sub('[a-zA-Z]+', '', text)\n",
        "  return new_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbhKsQ_bbBmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts = open('habr_texts.txt').read().splitlines()\n",
        "texts = [remove_english(remove_tags(text.lower())) for text in texts]\n",
        "texts = opt_normalize(texts, 30000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD_6fB-zc2c6",
        "colab_type": "text"
      },
      "source": [
        "Добавим tfidf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vgIcrzYc53f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToMxg8YMdua3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf = gensim.models.TfidfModel(id2word=dictionary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eorbB8Lge4nf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ph = gensim.models.Phrases(texts, scoring='npmi', threshold=0.8) \n",
        "p = gensim.models.phrases.Phraser(ph)\n",
        "nrammed_texts = p[texts]\n",
        "dictinary = gensim.corpora.Dictionary(texts)\n",
        "dictinary.filter_extremes(no_above=0.05, no_below=10)\n",
        "dictinary.compactify()\n",
        "corpus = [dictinary.doc2bow(text) for text in texts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TPGj6uBgPFo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "65c1e9af-7e73-48d4-fd44-4b5ff88def22"
      },
      "source": [
        "tfidf = gensim.models.TfidfModel(corpus, id2word=dictionary)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-c23ba4af21c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTfidfModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dictionary' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mv4wLmt0WxRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf = gensim.models.TfidfModel(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEVOTDRJd0sQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_topics = [50, 100, 200]\n",
        "n_eval = [0,1]\n",
        "best_res = 0\n",
        "best_params = {}\n",
        "best_topics = ''\n",
        "for n_topics in num_topics:\n",
        "  for n in n_eval:\n",
        "      lda = gensim.models.LdaMulticore(tfidf[corpus], n_topics, id2word=dictinary, eval_every=n)\n",
        "      coherence_model_lda = gensim.models.CoherenceModel(model=lda, \n",
        "                                                texts=texts, \n",
        "                                                  dictionary=dictinary, coherence='c_v')\n",
        "      if coherence_model_lda.get_coherence() > best_res:\n",
        "        best_res = coherence_model_lda.get_coherence()\n",
        "        best_params['topics'] = n_topics\n",
        "        best_params['eval'] = n\n",
        "        best_model = lda"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKNL7CNagiZ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "outputId": "0376b155-2e49-45a3-c977-9d89536823cc"
      },
      "source": [
        "best_model.print_topics()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(28,\n",
              "  '0.697*\"•\" + 0.233*\"±\" + 0.019*\"‘\" + 0.012*\"é\" + 0.009*\"́\" + 0.006*\"á\" + 0.005*\">\" + 0.005*\"`\" + 0.003*\"\\u200a\" + 0.002*\"„\"'),\n",
              " (20,\n",
              "  '0.515*\"‘\" + 0.290*\"`\" + 0.007*\"₽\" + 0.007*\"↑\" + 0.007*\"́\" + 0.007*\"ó\" + 0.007*\"±\" + 0.007*\"←\" + 0.007*\"é\" + 0.007*\"\\u200b\"'),\n",
              " (34,\n",
              "  '0.780*\"é\" + 0.054*\"€\" + 0.044*\"©\" + 0.038*\"№\" + 0.024*\"°\" + 0.020*\"▍\" + 0.011*\"\\u200a\" + 0.007*\"£\" + 0.005*\"±\" + 0.001*\"·\"'),\n",
              " (18,\n",
              "  '0.268*\"\\xad\" + 0.182*\"²\" + 0.159*\"€\" + 0.159*\"`\" + 0.064*\"°\" + 0.016*\"‘\" + 0.008*\"№\" + 0.006*\"±\" + 0.006*\"́\" + 0.006*\"\\u200b\"'),\n",
              " (30,\n",
              "  '0.374*\"ó\" + 0.234*\">\" + 0.143*\"\\u200b\" + 0.109*\"„\" + 0.080*\"№\" + 0.032*\"←\" + 0.010*\"^\" + 0.006*\"́\" + 0.003*\"`\" + 0.000*\"é\"'),\n",
              " (19,\n",
              "  '0.551*\">\" + 0.232*\"`\" + 0.103*\"№\" + 0.034*\"^\" + 0.019*\"▍\" + 0.019*\"±\" + 0.012*\"←\" + 0.007*\"€\" + 0.004*\"£\" + 0.004*\"₽\"'),\n",
              " (2,\n",
              "  '0.536*\"№\" + 0.151*\"±\" + 0.119*\"`\" + 0.065*\"°\" + 0.005*\"é\" + 0.005*\"↑\" + 0.005*\"́\" + 0.005*\"ó\" + 0.005*\"₽\" + 0.005*\"←\"'),\n",
              " (33,\n",
              "  '0.747*\"é\" + 0.046*\"←\" + 0.045*\"‘\" + 0.034*\"`\" + 0.028*\"±\" + 0.024*\"^\" + 0.023*\"×\" + 0.020*\"́\" + 0.010*\"°\" + 0.009*\"−\"'),\n",
              " (31,\n",
              "  '0.402*\"́\" + 0.224*\"£\" + 0.160*\"\\xad\" + 0.078*\"₽\" + 0.005*\"±\" + 0.005*\"↑\" + 0.005*\"ó\" + 0.005*\"²\" + 0.005*\"é\" + 0.005*\"\\u200b\"'),\n",
              " (37,\n",
              "  '0.176*\"£\" + 0.146*\"\\u2003\" + 0.146*\"≈\" + 0.146*\"„\" + 0.122*\"№\" + 0.090*\"×\" + 0.063*\">\" + 0.005*\"ó\" + 0.005*\"±\" + 0.005*\"·\"'),\n",
              " (25,\n",
              "  '0.464*\"‘\" + 0.157*\">\" + 0.083*\"^\" + 0.070*\"±\" + 0.059*\"²\" + 0.047*\"`\" + 0.026*\"á\" + 0.026*\"≈\" + 0.018*\"°\" + 0.003*\"\\u200a\"'),\n",
              " (43,\n",
              "  '0.156*\"ü\" + 0.133*\"\\xad\" + 0.133*\"↑\" + 0.115*\"←\" + 0.114*\"−\" + 0.111*\"®\" + 0.088*\"`\" + 0.071*\"©\" + 0.003*\"ó\" + 0.003*\"\\u200b\"'),\n",
              " (23,\n",
              "  '0.559*\"`\" + 0.155*\"↑\" + 0.133*\"é\" + 0.069*\"±\" + 0.003*\"₽\" + 0.003*\"́\" + 0.003*\"ó\" + 0.003*\"←\" + 0.003*\"©\" + 0.003*\"á\"'),\n",
              " (49,\n",
              "  '0.927*\"`\" + 0.019*\"°\" + 0.012*\"≈\" + 0.011*\"‘\" + 0.007*\"№\" + 0.005*\"−\" + 0.004*\"́\" + 0.004*\"^\" + 0.002*\"±\" + 0.002*\"ó\"'),\n",
              " (40,\n",
              "  '0.482*\"−\" + 0.135*\"°\" + 0.129*\"±\" + 0.048*\"é\" + 0.047*\"^\" + 0.043*\"ü\" + 0.036*\"\\u200b\" + 0.003*\"·\" + 0.003*\"́\" + 0.003*\"ó\"'),\n",
              " (39,\n",
              "  '0.782*\"°\" + 0.094*\"№\" + 0.031*\"‘\" + 0.024*\"·\" + 0.022*\"←\" + 0.014*\"²\" + 0.009*\"£\" + 0.005*\"±\" + 0.003*\"\\xad\" + 0.002*\"^\"'),\n",
              " (0,\n",
              "  '0.366*\"^\" + 0.250*\"„\" + 0.226*\"·\" + 0.123*\"\\u200a\" + 0.012*\"`\" + 0.008*\"©\" + 0.008*\"ü\" + 0.001*\"№\" + 0.000*\"é\" + 0.000*\"ó\"'),\n",
              " (8,\n",
              "  '0.339*\">\" + 0.301*\"\\u200a\" + 0.227*\"№\" + 0.023*\"®\" + 0.021*\"°\" + 0.019*\"„\" + 0.018*\"ó\" + 0.014*\"←\" + 0.002*\"é\" + 0.002*\"á\"'),\n",
              " (4,\n",
              "  '0.424*\"`\" + 0.187*\"ó\" + 0.098*\"‘\" + 0.070*\"≈\" + 0.070*\"±\" + 0.069*\"↑\" + 0.015*\"é\" + 0.008*\"№\" + 0.007*\">\" + 0.006*\"^\"'),\n",
              " (13,\n",
              "  '0.366*\"ó\" + 0.203*\"²\" + 0.193*\"№\" + 0.117*\"„\" + 0.046*\"^\" + 0.028*\"\\xad\" + 0.017*\"\\u200b\" + 0.010*\"‘\" + 0.005*\"±\" + 0.004*\"▍\"')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSRlIfTUoD58",
        "colab_type": "text"
      },
      "source": [
        "Модель решила ориентироваться на символы. Причём даже не на буквы, а просто на все подряд символы. Причём ей было очень грустно, так как некоторые символы в неё попали с весом 0\n",
        "'0.950*\"é\" + 0.042*\"•\" + 0.001*\"‘\" + 0.001*\"\\u2003\" + 0.000*\"←\" + 0.000*\"↑\" + 0.000*\"́\" + 0.000*\"ó\" + 0.000*\"²\" + 0.000*\"±\"'),\n",
        "Мне эти тематические модели совершенно не нравятся"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTmtE0hQjkvK",
        "colab_type": "text"
      },
      "source": [
        "Воспользуемся NMF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5Pa5nS0mNny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import NMF\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l8pMa7gr26b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stexts = [' '.join(text) for text in texts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQl9MqZJOVrq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "min_error = math.inf\n",
        "for max_f in [500, 1000]:\n",
        "  vectorizer = TfidfVectorizer(max_features=max_f)\n",
        "  X = vectorizer.fit_transform(stexts)\n",
        "  for a in [0, 1]:\n",
        "    for n in [20, 40]:\n",
        "      model = NMF(n_components=n, alpha=a)\n",
        "      model.fit(X)\n",
        "      error = model.reconstruction_err_\n",
        "      if error < min_error:\n",
        "        min_error = error\n",
        "        best_model = model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqbpmYTKvEeg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "outputId": "3a127cd9-8603-43f4-f8b8-84543774a1b6"
      },
      "source": [
        "top_words = best_model.components_.argsort()[:,:-5:-1]\n",
        "\n",
        "feat_names = vectorizer.get_feature_names()\n",
        "\n",
        "for i in range(top_words.shape[0]):\n",
        "    words = [feat_names[j] for j in top_words[i]]\n",
        "    print(i, \"  \".join(words))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 наш  литр  метр  далее\n",
            "1 далее  литр  наш  плата\n",
            "2 метр  далее  год  литр\n",
            "3 плата  год  метр  наш\n",
            "4 год  плата  литр  метр\n",
            "5 плата  метр  наш  число\n",
            "6 наш  число  плата  метр\n",
            "7 наш  литр  год  число\n",
            "8 далее  литр  наш  число\n",
            "9 метр  далее  плата  число\n",
            "10 год  число  литр  метр\n",
            "11 плата  далее  год  метр\n",
            "12 плата  наш  далее  метр\n",
            "13 число  литр  метр  плата\n",
            "14 год  число  плата  далее\n",
            "15 число  метр  наш  плата\n",
            "16 литр  метр  год  плата\n",
            "17 метр  литр  плата  наш\n",
            "18 год  наш  далее  метр\n",
            "19 плата  метр  литр  число\n",
            "20 литр  метр  наш  плата\n",
            "21 наш  далее  метр  плата\n",
            "22 год  плата  число  далее\n",
            "23 число  литр  плата  наш\n",
            "24 число  год  метр  плата\n",
            "25 литр  плата  далее  наш\n",
            "26 литр  год  число  плата\n",
            "27 наш  литр  год  метр\n",
            "28 плата  наш  число  метр\n",
            "29 число  год  плата  далее\n",
            "30 плата  литр  год  далее\n",
            "31 далее  число  плата  метр\n",
            "32 число  далее  наш  метр\n",
            "33 метр  литр  плата  наш\n",
            "34 литр  метр  наш  плата\n",
            "35 плата  наш  метр  литр\n",
            "36 наш  метр  год  далее\n",
            "37 далее  наш  плата  число\n",
            "38 литр  далее  метр  плата\n",
            "39 число  литр  плата  наш\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grVjv9ulRopp",
        "colab_type": "text"
      },
      "source": [
        "Получились ужасные темы, потому что tfidf большую часть слов решил не рассматривать. Когда я пыталась настраивать пороги (в том числе предлагала ему 0 и плюс бесконечность), он выражал недовольство, так как не мог найти ни одного текста ни с какими порогами. Ещё была выдача, состоящая исключительно из комбинаций слов \"год\" и \"число\". "
      ]
    }
  ]
}