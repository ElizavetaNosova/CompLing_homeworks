{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "from nltk import sent_tokenize\n",
    "punctuation += \"«»—…“”\"\n",
    "punct = set(punctuation)\n",
    "import copy\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = open('sents_with_mistakes.txt.txt', encoding='utf8').read().splitlines()\n",
    "true = open('correct_sents.txt.txt', encoding='utf8').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пояним эту мысль.\n",
      "Поясним эту мысль\n"
     ]
    }
   ],
   "source": [
    "print(bad[2])\n",
    "print(true[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sents(sents):\n",
    "    tokenized = []\n",
    "    for sent in sents:\n",
    "        sent = sent.lower()\n",
    "        tokens = sent.split()\n",
    "        tokens = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens if (set(token)-punct)]\n",
    "        tokenized.append(list(tokens))\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_tokens = prepare_sents(bad)\n",
    "true_tokens = prepare_sents(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['опофеозом', 'дня', 'для', 'меня', 'сегодня', 'стала', 'фраза', 'услышанная', 'в', 'новостях']\n"
     ]
    }
   ],
   "source": [
    "print(bad_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('corpus_5000.txt', 'w', encoding = 'utf-8')\n",
    "with gzip.open('lenta-ru-news.csv.gz', 'rt', encoding='utf-8') as archive:\n",
    "    reader = csv.reader(archive, delimiter=',', quotechar='\"')\n",
    "    for i, line in enumerate(reader):\n",
    "        if i < 5000: \n",
    "            corpus.write(line[2].replace('\\xa0', ' ') + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    normalized_text = [(word.strip(punctuation)) for word in text.lower().split()]\n",
    "    normalized_text = [word for word in normalized_text if word]\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lizan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "punctuation = string.punctuation+ \"«»—…“”\"\n",
    "punct = set(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for text in open('corpus_5000.txt', encoding = 'utf-8').read().splitlines():\n",
    "    sents = sent_tokenize(text)\n",
    "    norm_sents = [normalize(sent) for sent in sents]\n",
    "    corpus += norm_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_from_multiplied_letters(text):  #большое количество повторяющихся букв превращается в две повторяющиеся буквы\n",
    "    if isinstance(text, list):            \n",
    "        for i in text:               \n",
    "            i = clear_from_multiplied_letters(i) \n",
    "    elif isinstance(text, str):\n",
    "        symbols_to_delete = []\n",
    "        for i in range(len(text)-2):\n",
    "              if text[i] == text[i+1] and text[i] == text[i+2]:\n",
    "                symbols_to_delete.append(i)\n",
    "        if len(symbols_to_delete) > 0:\n",
    "            textlist = list(text)\n",
    "            for i in range(len(symbols_to_delete)):\n",
    "                textlist.pop(-i-1)       #иду от конца к началу, чтобы не сдвигались индексы\n",
    "                text = ''.join(textlist)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "хороооо\n"
     ]
    }
   ],
   "source": [
    "print(clear_from_multiplied_letters('хорошоооооо'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "хорошоо\n"
     ]
    }
   ],
   "source": [
    "print(clear_from_multiplied_letters('хорошооо'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Это очень странно, почему-то убирается только одна из разноженных букв. Простите, не успела это исправить.\n",
    "#В таком виде функция всё же чуть-чуть улучшает ситуацию, поэтому применю"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_multyclean = clear_from_multiplied_letters(bad_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_letter(word, n):\n",
    "    word_L = word[:n]\n",
    "    word_R = word[n:]\n",
    "    word_ufterdel = word_L + word_R[1:]\n",
    "    return word_ufterdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_del(corpus): #корпус должен быть представлен как список токеноd\n",
    "    corpusdict = defaultdict(set)\n",
    "    for word in corpus:\n",
    "        for n in range(len(word)):\n",
    "            word1del = del_letter(word, n)\n",
    "            corpusdict[word1del].add(word)\n",
    "            if len(word) >= 2:\n",
    "                 for n2 in range(n):\n",
    "                    word2del = del_letter(word1del, n2)\n",
    "                    corpusdict[word2del].add(word)\n",
    "    return(corpusdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_list = []\n",
    "for sent in corpus:\n",
    "    for word in sent:\n",
    "        corpus_list.append(word)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusdict = corpus_del(corpus_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'склока', 'кнопка', 'кошкам', 'конкау', 'комика', 'конька', 'кошка', 'койках', 'козака', 'кокаин', 'кокату'}\n"
     ]
    }
   ],
   "source": [
    "print(corpusdict['кока'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symspell2dels(text, corpus, corpusdict):\n",
    "    corrections = {}\n",
    "    for word in text:\n",
    "        if word not in corpus:\n",
    "            correct = set()\n",
    "            delword = []\n",
    "            delword.append(word)\n",
    "            for n in range(len(word)):\n",
    "                word1del = del_letter(word, n)\n",
    "                delword.append(word1del)\n",
    "                for n2 in range(n):\n",
    "                    word2del = del_letter(word1del, n)\n",
    "                    delword.append(word2del)     \n",
    "            for seq in delword:\n",
    "                if seq in corpus:\n",
    "                    correct.append(seq)\n",
    "                if seq in corpusdict:\n",
    "                    correct.add(optiondict[seq])\n",
    "        corrections[word] = correct\n",
    "    return corrections  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_list = []\n",
    "for i in bad_multyclean:\n",
    "    for j in bad:\n",
    "        bad_list.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections = symspell2dels(bad_list, corpus, corpusdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#судя по времени выполнения предыдущей строки, удалять по 2 буквы было стратегической ошибкой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_corrections_distance(corrections):\n",
    "    corretions_1option = copy.deepcopy(corrections)\n",
    "    for word in corrections_1 option:\n",
    "        if len(correction_1option[word]) > 1:\n",
    "            correction_1option[word]= get_closest_match_with_metric(word, correction_1option[word], 1, textdistance.damerau_levenshtein)\n",
    "    return corrections_1option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections_1option = choose_corrections_distance(corrections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_using_dist = copy.deepcopy(bad)\n",
    "for sentence in corrected_using_dist:\n",
    "    for n in len(sentence):\n",
    "        word = sentence[n]\n",
    "        if word in corrections_1option:\n",
    "            sentence[n] = sentence[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценим работу спелчекера, выбирающего одно из наиболее близких слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(true, bad, corrected):\n",
    "    metrics_dict = {}\n",
    "    metrics_dict[total_mistaken] = 0\n",
    "    metrics_dict[really_corrected] = 0\n",
    "    metrics_dict[not_corrected] = 0\n",
    "    metrics_dict[broken] = 0\n",
    "    for i in range(len(true)):\n",
    "        for j in range(len(true[i])):\n",
    "            true_word = true[i][j]\n",
    "            bad_word = bad[i][j]\n",
    "            corrected_word = corrected[i][j]\n",
    "            if true_word != bad_word:\n",
    "                if true_word == corrected_word:\n",
    "                    metrics_dict[really_corrected] += 1\n",
    "                else:\n",
    "                    metrics_dict[total_mistaken] += 1\n",
    "                    metrics_dict[not_corrected] += 1\n",
    "            else:\n",
    "                if true_word != corrected_word:\n",
    "                    metrics_dict[total_mistaken] += 1\n",
    "                    metrics_dict[broken] += 1\n",
    "    return(metrics_dict)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Будем выбирать слово в соответствии с частотностью триграм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = Counter()\n",
    "trigrams = Counter()\n",
    "\n",
    "corpus_forNgtams = copy.deepcopy(corpus)\n",
    "\n",
    "for sentence in corpus_forNgrams:\n",
    "    sentence_forNgram = [<START>, <START>]\n",
    "    for word in sentence:\n",
    "        sentence_forNgram.append(word)\n",
    "    bigrams.update(ngrammer(sentence_forNgram))\n",
    "    trigrams.update(ngrammer(sentence_forNgram, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_correct(text, corrections, bigrams, bigrams):\n",
    "    corrected_text = copy.deepcopy(text)\n",
    "    for n_sent in range(len(text)):\n",
    "        for i in range(2, len(sentence)):\n",
    "            if word in corrections:\n",
    "                wordset = corrections[word]\n",
    "                if len() == 1:\n",
    "                    corrected_text[n_sent][i] = wordset.pop()\n",
    "                else:\n",
    "                    corrected_prev1 = corrected_text[n_sent][i-1]  #брать одно слово из исправленного текста, а одно из изначального, бесполезно:\n",
    "                    corrected_prev2 = corrected_text[n_sent][i-2]  #если какое-то из предыдущих двух слов было исправлено, значит, его нет в корпусе,\n",
    "                    text_prev1 = text[n_sent][i-1]                #в таком случае вероятность триграмы и биграмы с изначальным словом равна нулю\n",
    "                    text_prev2 = text[n_sent][i-2]\n",
    "                    possible_prev = set(corrected_prev2 + ' ' + corrected_prev1, text_prev2 + ' ' + text_prev1)\n",
    "                    max_p = 0              \n",
    "                    max_p_words = []\n",
    "                    for option in wordset:\n",
    "                        for bigram in possible_prev:\n",
    "                            trigram = bigram + ' ' + option\n",
    "                            try:\n",
    "                                p = trigrams[trigram]/bigrams[bigram]\n",
    "                            except:\n",
    "                                p = 0\n",
    "                            if p == max_p:\n",
    "                                max_p_words.append(option)\n",
    "                            elif p > max_p:\n",
    "                                max_p_words = []\n",
    "                            max_p = p\n",
    "                if len(max_p_words) = 1:\n",
    "                     corrected_text[n_sent][i] = max_p_words[0]\n",
    "                elif len(max_p_words) > 1: #если есть несколько равновероятных кандидатов (например, если предшествующей биграмы в корпусе не было),\n",
    "                    chosen = None         #выбираем самое близкое\n",
    "                    for cand in max_p_words: \n",
    "                        chosen= get_closest_match_with_metric(word, max_p_words, 1, textdistance.damerau_levenshtein)\n",
    "                    corrected_text[n_sent][i] = chosen   \n",
    "    return corrected_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
