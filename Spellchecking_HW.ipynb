{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "from nltk import sent_tokenize\n",
    "punctuation += \"«»—…“”\"\n",
    "punct = set(punctuation)\n",
    "import copy\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = open('sents_with_mistakes.txt.txt', encoding='utf8').read().splitlines()\n",
    "true = open('correct_sents.txt.txt', encoding='utf8').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пояним эту мысль.\n",
      "Поясним эту мысль\n"
     ]
    }
   ],
   "source": [
    "print(bad[2])\n",
    "print(true[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sents(sents):\n",
    "    tokenized = []\n",
    "    for sent in sents:\n",
    "        sent = sent.lower()\n",
    "        tokens = sent.split()\n",
    "        tokens = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens if (set(token)-punct)]\n",
    "        tokenized.append(list(tokens))\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_tokens = prepare_sents(bad)\n",
    "true_tokens = prepare_sents(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['опофеозом', 'дня', 'для', 'меня', 'сегодня', 'стала', 'фраза', 'услышанная', 'в', 'новостях']\n"
     ]
    }
   ],
   "source": [
    "print(bad_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('corpus_5000.txt', 'w', encoding = 'utf-8')\n",
    "with gzip.open('lenta-ru-news.csv.gz', 'rt', encoding='utf-8') as archive:\n",
    "    reader = csv.reader(archive, delimiter=',', quotechar='\"')\n",
    "    for i, line in enumerate(reader):\n",
    "        if i < 5000: \n",
    "            corpus.write(line[2].replace('\\xa0', ' ') + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    normalized_text = [(word.strip(punctuation)) for word in text.lower().split()]\n",
    "    normalized_text = [word for word in normalized_text if word]\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lizan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "punctuation = string.punctuation+ \"«»—…“”\"\n",
    "punct = set(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for text in open('corpus_5000.txt', encoding = 'utf-8').read().splitlines():\n",
    "    sents = sent_tokenize(text)\n",
    "    norm_sents = [['<START>'] + ['<START>'] + normalize(sent) + ['<END>'] + ['<END>'] for sent in sents]\n",
    "    corpus += norm_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Нам всё равно понадобится список токенов без повторений, почему бы не посчитать частотность слов и заодно Ngram уже сейчас"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams= Counter()\n",
    "bigrams = Counter()\n",
    "trigrams = Counter()\n",
    "\n",
    "for sentence in corpus:\n",
    "    unigrams.update(sentence)\n",
    "    bigrams.update(ngrammer(sentence))\n",
    "    trigrams.update(ngrammer(sentence, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_letter(word, n=0):\n",
    "    word_L = word[:n]\n",
    "    word_R = word[n:]\n",
    "    word_afterdel = word_L + word_R[1:]\n",
    "    return word_afterdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_multiplied(word): #один раз превращает три повторяющиеся буквы в две\n",
    "    delword = []\n",
    "    clear_word = word\n",
    "    for i in range(len(word)-3):\n",
    "        if word[i] == word[i+1] and word[i] == word[i+2]:\n",
    "            clear_word = del_letter(clear_word, i) \n",
    "            break\n",
    "    return clear_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "хорошооооо\n"
     ]
    }
   ],
   "source": [
    "print(clear_multiplied('хорошоооооо')) #убирает часть из повторяющихся букв"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_all_multiplied(word):\n",
    "    word2 = word\n",
    "    while clear_multiplied(clear_multiplied(word2)) != clear_multiplied(word2):\n",
    "        word2 = clear_multiplied(clear_multiplied(word2))\n",
    "    return word2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "оччччень оччень\n",
      "каааак каак\n",
      "оооочень оочень\n",
      "оооочень оочень\n",
      "страааашно страашно\n",
      "оооочень оочень\n",
      "оооочень оочень\n",
      "доолго-дооолго-доооолго доолго-доолго-дооолго\n",
      "оооочень оочень\n",
      "мааааммоооочкииии мааммоочкииии\n",
      "ээээххх ээххх\n",
      "ооооочень ооочень\n",
      "ооооочень ооочень\n",
      "ооооочень ооочень\n",
      "оооочень оочень\n",
      "ооооочень ооочень\n",
      "прррроклятые прроклятые\n",
      "каааак каак\n",
      "оооочень оочень\n",
      "ооооочень ооочень\n",
      "оооочень оочень\n",
      "спаааасиб спаасиб\n",
      "оооочень оочень\n",
      "кошшшшка кошшка\n",
      "каааак каак\n",
      "воооот воот\n",
      "покажиииии покажиии\n"
     ]
    }
   ],
   "source": [
    "#прошу прощения, я так и не поняла, почему функция убрает не все случаи, когда подряд идёт юольше двух одинаковых символов,\n",
    "#но часть она убирает, так что применю. В примере выше все буквы о точно кириллические.\n",
    "bad_tokens_multiclean = bad_tokens\n",
    "for i in bad_tokens_multiclean:\n",
    "    for token in i:\n",
    "        new_token = clear_all_multiplied(token)\n",
    "        if new_token != token:\n",
    "            print(token, new_token)\n",
    "            token = new_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_del(corpus):\n",
    "    corpusdict = defaultdict(set)\n",
    "    for word in corpus:\n",
    "        for n in range(len(word)):\n",
    "            word1del = del_letter(word, n)\n",
    "            corpusdict[word1del].add(word)\n",
    "    return(corpusdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusdict = corpus_del(unigrams.elements())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'кожа', 'копа', 'коба', 'кота', 'коза', 'коап', 'кора', 'кома', 'кода', 'коса'}\n"
     ]
    }
   ],
   "source": [
    "print(corpusdict['коа'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_set = set(unigrams.elements())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symspell2dels(text, corpus, corpusdict):\n",
    "    textset = set(text)\n",
    "    corrections = {}\n",
    "    for word in textset:\n",
    "        correct = set()\n",
    "        if word not in corpus:\n",
    "            delword = set()\n",
    "            for n in range(len(word)):\n",
    "                word1del = del_letter(word, n)\n",
    "                delword.add(word1del) \n",
    "            for seq in delword:\n",
    "                if seq in corpus:\n",
    "                    correct.add(seq)\n",
    "                if seq in corpusdict:\n",
    "                    for correctword in corpusdict[seq]:\n",
    "                        correct.add(correctword)\n",
    "        if correct:\n",
    "            corrections[word] = correct\n",
    "    return corrections  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_list = []\n",
    "for i in bad_tokens_multiclean:\n",
    "    for j in i:\n",
    "        bad_list.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections = symspell2dels(bad_list, unigrams_set, corpusdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(bad_tokens_multiclean)\n",
    "corrected_randomoption = [[] for i in range(n)]\n",
    "for i in range(n):\n",
    "    for token in bad_tokens_multiclean[i]:\n",
    "        if token not in corrections:\n",
    "                corrected_randomoption[i].append(token)\n",
    "        else:\n",
    "            existing_word = random.choice(list(corrections[token]))\n",
    "            corrected_randomoption[i].append(existing_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['апофеозом', 'дня', 'для', 'меня', 'сегодня', 'стала', 'фраза', 'услышанная', 'в', 'новостях']\n"
     ]
    }
   ],
   "source": [
    "print(corrected_randomoption[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценим работу спелчекера, выбирающего одно из наиболее близких слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(true, bad, corrected):\n",
    "    total_mistaken = 0\n",
    "    really_corrected = 0\n",
    "    not_corrected = 0\n",
    "    broken = 0\n",
    "    correct_at_bad = 0\n",
    "    wrong_at_bad = 0\n",
    "    for i in range(len(true)):\n",
    "        if len(true[i]) == len(bad[i]): #я не исправляла ошибки в слитно-раздельном написании\n",
    "            for j in range(len(true[i])):\n",
    "                true_word = true[i][j]\n",
    "                bad_word = bad[i][j]\n",
    "                corrected_word = corrected[i][j]\n",
    "                if true_word != bad_word:\n",
    "                    wrong_at_bad += 1\n",
    "                    if true_word == corrected_word:\n",
    "                        really_corrected += 1\n",
    "                    else:\n",
    "                        total_mistaken += 1\n",
    "                        not_corrected += 1\n",
    "                else:\n",
    "                    correct_at_bad += 1\n",
    "                    if true_word != corrected_word:\n",
    "                        total_mistaken += 1\n",
    "                        broken += 1\n",
    "    metrics_dict = {}\n",
    "    metrics_dict['%mistakes'] = total_mistaken/(correct_at_bad + wrong_at_bad)\n",
    "    metrics_dict['%corrected'] = really_corrected/wrong_at_bad * 100\n",
    "    metrics_dict['%broken'] = broken/correct_at_bad*100\n",
    "    return(metrics_dict)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_metrics_random = metrics(true_tokens, bad_tokens, corrected_randomoption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'%mistakes': 0.1567783891945973, '%corrected': 27.251552795031053, '%broken': 7.235557597335478}\n"
     ]
    }
   ],
   "source": [
    "print(dist_metrics_random) #при перезапуске кода проценты будут немного меняться"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Будем выбирать слово в соответствии с частотностью триграм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N-грамы посчитали выше\n",
    "def p_max_words(ngrams, prevngrams, prevngram, options, p_max0 = 0, p_max_words0 = []):\n",
    "    p_max_words = p_max_words0\n",
    "    p_max = p_max0\n",
    "    for option in options:\n",
    "        try:\n",
    "            p = ngrams[prevngram + ' ' + option]/prevngrams[prevngram]\n",
    "        except ZeroDivisionError:\n",
    "                p = 0\n",
    "        if p > p_max:\n",
    "            p_max = p\n",
    "            p_max_words = []\n",
    "        if p == p_max:\n",
    "            p_max_words.append(option)\n",
    "    return(p_max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_correct_sentence(sentence, corrections, bigrams, trigrams):\n",
    "    corrected_sent = ['<start>', '<start>']\n",
    "    for word in sentence:\n",
    "        if word not in corrections:\n",
    "            corrected_sent.append(word)\n",
    "        else:\n",
    "            correction = list(corrections[word])\n",
    "            if len(correction) == 1:\n",
    "                corrected_sent.append(correction[0])\n",
    "            else:\n",
    "                try:\n",
    "                    corrected_bigram = corrected_sent[-2] + ' ' + corrected_sent[-1]\n",
    "                except:\n",
    "                    print(isinstance(corrected_sent[-2] , str))\n",
    "                max_p = 0\n",
    "                max_p_words = []\n",
    "                p_max_trigram = p_max_words(trigrams, bigrams, corrected_bigram, correction)\n",
    "                try:\n",
    "                    chosen = random.choice(list(p_max_trigram))  #берём любое слово из самых вероятных\n",
    "                    corrected_sent.append(chosen)\n",
    "                except:\n",
    "                    corrected_sent.append(word)  #если по какой-то причине список кандидатов оказался пустым,\n",
    "    return corrected_sent                           #добавится изначальное слово"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_text_trigrams = [trigram_correct_sentence(sent, corrections, bigrams, trigrams)[2:] for sent in bad_tokens]\n",
    "#не берём тэги начала предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'%mistakes': 0.16258129064532267, '%corrected': 22.748447204968944, '%broken': 7.235557597335478}\n"
     ]
    }
   ],
   "source": [
    "dist_metrics_trigram = metrics(true_tokens, bad_tokens, corrected_text_trigrams)\n",
    "print(dist_metrics_trigram) \n",
    "#процент broken не изменился, так как для этой метрики неважно, на что заменено правильное слово\n",
    "#процент corrected понизился"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
