{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Embeddings.ipynbgsHW",
      "provenance": [],
      "authorship_tag": "ABX9TyORY9w2M317YUdwtK4OUm9P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElizavetaNosova/CompLing_homeworks/blob/master/Embeddings_ipynbgsHW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj44O65Alsiy",
        "colab_type": "code",
        "outputId": "2463e434-a78a-4deb-e68b-ef7b915d56f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        }
      },
      "source": [
        "! pip install pymorphy2[fast]\n",
        "! pip install gensim\n",
        "! pip install sklearn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pymorphy2[fast]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
            "\r\u001b[K     |███████                         | 10kB 17.0MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 30kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 1.9MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Collecting pymorphy2-dicts<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2[fast]) (0.6.2)\n",
            "Collecting DAWG>=0.7.3; extra == \"fast\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/c0/d8d967bcaa0b572f9dc1d878bbf5a7bfd5afa2102a5ae426731f6ce3bc26/DAWG-0.7.8.tar.gz (255kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 50.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: DAWG\n",
            "  Building wheel for DAWG (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for DAWG: filename=DAWG-0.7.8-cp36-cp36m-linux_x86_64.whl size=774827 sha256=fde1449e74d3c20b4113dbc015fb3e5face09c48291adff005f5129fb4b46b84\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/88/d0/4e4abc83eb8f59a71e8dbd8ba99fd5615a3af1fac1ef7f8825\n",
            "Successfully built DAWG\n",
            "Installing collected packages: dawg-python, pymorphy2-dicts, DAWG, pymorphy2\n",
            "Successfully installed DAWG-0.7.8 dawg-python-0.7.2 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.9.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.17.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.11.15)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.21.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.4)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->smart-open>=1.2.1->gensim) (2.6.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.14.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.17.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA0tRTYHI8Rg",
        "colab_type": "code",
        "outputId": "d81c822d-5284-4f5f-ed19-343679187d22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import pandas as pd\n",
        "from lxml import html\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import gensim\n",
        "import numpy as np\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from collections import Counter,defaultdict\n",
        "from string import punctuation\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "%matplotlib inline\n",
        "\n",
        "morph = MorphAnalyzer()\n",
        "punct = punctuation+'«»—…“”*№–'\n",
        "stops = set(stopwords.words('russian'))\n",
        "\n",
        "def normalize(text):\n",
        "    \n",
        "    words = [word.strip(punct) for word in text.lower().split()]\n",
        "    words = [morph.parse(word)[0].normal_form for word in words if word and word not in stops]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "def tokenize(text):\n",
        "    \n",
        "    words = [word.strip(punct) for word in text.lower().split()]\n",
        "\n",
        "    return ' '.join(words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGwYbGctl7ff",
        "colab_type": "text"
      },
      "source": [
        "1) Векторизуйте тексты с помощью Word2vec модели, обученной самостоятельно, и с помощью модели, взятой с rusvectores (любой). Обучите 2 модели по определению перефразирования на получившихся векторах и проверьте, что работает лучше. \n",
        "Word2Vec нужно обучить на отдельном корпусе (не на парафразах). Можно взять данные из семинара или любые другие. \n",
        "ВАЖНО: Оценивать модели нужно с помощью кросс-валидации! Метрика - f1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OOptv3tm-uv",
        "colab_type": "code",
        "outputId": "0339b291-3667-4f7a-d431-a0f175fcf1b2",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-01da4336-39ee-49b5-a7d3-925f4284dbcd\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-01da4336-39ee-49b5-a7d3-925f4284dbcd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving lenta1000.csv to lenta1000.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxD5aIL36EtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "lenta = pd.read_csv('lenta1000.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx5KueorSGFC",
        "colab_type": "text"
      },
      "source": [
        "Предполагаю, что на тысяче текстов качество будет так себе, но большой архив упорно не хотел загружаться на колаб. Надеюсь, что структура новостей в ленте (заголовок - полноцнное предложение, у которого есть синонимы в тексте) поможет выявить сколько-то синонимов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAHa5Fk6PWi_",
        "colab_type": "code",
        "outputId": "aa3b9c07-fcdd-475f-d0b6-d553316656b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "source": [
        "lenta"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>url</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>topic</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>https://lenta.ru/news/2018/12/14/cancer/</td>\n",
              "      <td>Названы регионы России с самой высокой смертно...</td>\n",
              "      <td>Вице-премьер по социальным вопросам Татьяна Го...</td>\n",
              "      <td>Россия</td>\n",
              "      <td>Общество</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>https://lenta.ru/news/2018/12/15/doping/</td>\n",
              "      <td>Австрия не представила доказательств вины росс...</td>\n",
              "      <td>Австрийские правоохранительные органы не предс...</td>\n",
              "      <td>Спорт</td>\n",
              "      <td>Зимние виды</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>https://lenta.ru/news/2018/12/15/disneyland/</td>\n",
              "      <td>Обнаружено самое счастливое место на планете</td>\n",
              "      <td>Сотрудники социальной сети Instagram проанализ...</td>\n",
              "      <td>Путешествия</td>\n",
              "      <td>Мир</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>https://lenta.ru/news/2018/12/15/usa25/</td>\n",
              "      <td>В США раскрыли сумму расходов на расследование...</td>\n",
              "      <td>С начала расследования российского вмешательст...</td>\n",
              "      <td>Мир</td>\n",
              "      <td>Политика</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>https://lenta.ru/news/2018/12/15/integrity/</td>\n",
              "      <td>Хакеры рассказали о планах Великобритании зами...</td>\n",
              "      <td>Хакерская группировка Anonymous опубликовала н...</td>\n",
              "      <td>Мир</td>\n",
              "      <td>Общество</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>995</td>\n",
              "      <td>https://lenta.ru/news/2018/12/08/skater/</td>\n",
              "      <td>Американский фигурист «воткнул» партнершу в ле...</td>\n",
              "      <td>Американский фигурист Тимоти Ледук уронил парт...</td>\n",
              "      <td>Спорт</td>\n",
              "      <td>Зимние виды</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>996</td>\n",
              "      <td>https://lenta.ru/news/2018/12/08/vred/</td>\n",
              "      <td>В новой нефтяной сделке нашли вред для России</td>\n",
              "      <td>Новое соглашение членов Организации стран-эксп...</td>\n",
              "      <td>Экономика</td>\n",
              "      <td>Госэкономика</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>997</td>\n",
              "      <td>https://lenta.ru/news/2018/12/08/9m729/</td>\n",
              "      <td>Предсказано будущее ракеты 9М729 после ультима...</td>\n",
              "      <td>Ответом России на ультиматум США по Договору о...</td>\n",
              "      <td>Наука и техника</td>\n",
              "      <td>Оружие</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>998</td>\n",
              "      <td>https://lenta.ru/news/2018/12/08/paris/</td>\n",
              "      <td>В охваченный протестами Париж вошла бронетехника</td>\n",
              "      <td>В Париж, где продолжаются антиправительственны...</td>\n",
              "      <td>Мир</td>\n",
              "      <td>Общество</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>999</td>\n",
              "      <td>https://lenta.ru/news/2018/12/08/pribavka/</td>\n",
              "      <td>Униженный доплатой ветеран пообещал отправить ...</td>\n",
              "      <td>Пенсионер Владимир Попов сообщил, что обращалс...</td>\n",
              "      <td>Россия</td>\n",
              "      <td>Общество</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Unnamed: 0  ...          tags\n",
              "0             0  ...      Общество\n",
              "1             1  ...   Зимние виды\n",
              "2             2  ...           Мир\n",
              "3             3  ...      Политика\n",
              "4             4  ...      Общество\n",
              "..          ...  ...           ...\n",
              "995         995  ...   Зимние виды\n",
              "996         996  ...  Госэкономика\n",
              "997         997  ...        Оружие\n",
              "998         998  ...      Общество\n",
              "999         999  ...      Общество\n",
              "\n",
              "[1000 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjK5ucgFPeDj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lenta['title_and_text'] = lenta.title + ' ' + lenta.text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ednx5t1MPt55",
        "colab_type": "code",
        "outputId": "04af2d0d-53e0-4c7f-db61-31cd4c1dfb88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        }
      },
      "source": [
        "lenta"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>url</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>topic</th>\n",
              "      <th>tags</th>\n",
              "      <th>title_and_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>https://lenta.ru/news/2018/12/14/cancer/</td>\n",
              "      <td>Названы регионы России с самой высокой смертно...</td>\n",
              "      <td>Вице-премьер по социальным вопросам Татьяна Го...</td>\n",
              "      <td>Россия</td>\n",
              "      <td>Общество</td>\n",
              "      <td>Названы регионы России с самой высокой смертно...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>https://lenta.ru/news/2018/12/15/doping/</td>\n",
              "      <td>Австрия не представила доказательств вины росс...</td>\n",
              "      <td>Австрийские правоохранительные органы не предс...</td>\n",
              "      <td>Спорт</td>\n",
              "      <td>Зимние виды</td>\n",
              "      <td>Австрия не представила доказательств вины росс...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>https://lenta.ru/news/2018/12/15/disneyland/</td>\n",
              "      <td>Обнаружено самое счастливое место на планете</td>\n",
              "      <td>Сотрудники социальной сети Instagram проанализ...</td>\n",
              "      <td>Путешествия</td>\n",
              "      <td>Мир</td>\n",
              "      <td>Обнаружено самое счастливое место на планете С...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>https://lenta.ru/news/2018/12/15/usa25/</td>\n",
              "      <td>В США раскрыли сумму расходов на расследование...</td>\n",
              "      <td>С начала расследования российского вмешательст...</td>\n",
              "      <td>Мир</td>\n",
              "      <td>Политика</td>\n",
              "      <td>В США раскрыли сумму расходов на расследование...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>https://lenta.ru/news/2018/12/15/integrity/</td>\n",
              "      <td>Хакеры рассказали о планах Великобритании зами...</td>\n",
              "      <td>Хакерская группировка Anonymous опубликовала н...</td>\n",
              "      <td>Мир</td>\n",
              "      <td>Общество</td>\n",
              "      <td>Хакеры рассказали о планах Великобритании зами...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>995</td>\n",
              "      <td>https://lenta.ru/news/2018/12/08/skater/</td>\n",
              "      <td>Американский фигурист «воткнул» партнершу в ле...</td>\n",
              "      <td>Американский фигурист Тимоти Ледук уронил парт...</td>\n",
              "      <td>Спорт</td>\n",
              "      <td>Зимние виды</td>\n",
              "      <td>Американский фигурист «воткнул» партнершу в ле...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>996</td>\n",
              "      <td>https://lenta.ru/news/2018/12/08/vred/</td>\n",
              "      <td>В новой нефтяной сделке нашли вред для России</td>\n",
              "      <td>Новое соглашение членов Организации стран-эксп...</td>\n",
              "      <td>Экономика</td>\n",
              "      <td>Госэкономика</td>\n",
              "      <td>В новой нефтяной сделке нашли вред для России ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>997</td>\n",
              "      <td>https://lenta.ru/news/2018/12/08/9m729/</td>\n",
              "      <td>Предсказано будущее ракеты 9М729 после ультима...</td>\n",
              "      <td>Ответом России на ультиматум США по Договору о...</td>\n",
              "      <td>Наука и техника</td>\n",
              "      <td>Оружие</td>\n",
              "      <td>Предсказано будущее ракеты 9М729 после ультима...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>998</td>\n",
              "      <td>https://lenta.ru/news/2018/12/08/paris/</td>\n",
              "      <td>В охваченный протестами Париж вошла бронетехника</td>\n",
              "      <td>В Париж, где продолжаются антиправительственны...</td>\n",
              "      <td>Мир</td>\n",
              "      <td>Общество</td>\n",
              "      <td>В охваченный протестами Париж вошла бронетехни...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>999</td>\n",
              "      <td>https://lenta.ru/news/2018/12/08/pribavka/</td>\n",
              "      <td>Униженный доплатой ветеран пообещал отправить ...</td>\n",
              "      <td>Пенсионер Владимир Попов сообщил, что обращалс...</td>\n",
              "      <td>Россия</td>\n",
              "      <td>Общество</td>\n",
              "      <td>Униженный доплатой ветеран пообещал отправить ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Unnamed: 0  ...                                     title_and_text\n",
              "0             0  ...  Названы регионы России с самой высокой смертно...\n",
              "1             1  ...  Австрия не представила доказательств вины росс...\n",
              "2             2  ...  Обнаружено самое счастливое место на планете С...\n",
              "3             3  ...  В США раскрыли сумму расходов на расследование...\n",
              "4             4  ...  Хакеры рассказали о планах Великобритании зами...\n",
              "..          ...  ...                                                ...\n",
              "995         995  ...  Американский фигурист «воткнул» партнершу в ле...\n",
              "996         996  ...  В новой нефтяной сделке нашли вред для России ...\n",
              "997         997  ...  Предсказано будущее ракеты 9М729 после ультима...\n",
              "998         998  ...  В охваченный протестами Париж вошла бронетехни...\n",
              "999         999  ...  Униженный доплатой ветеран пообещал отправить ...\n",
              "\n",
              "[1000 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC4YZEZtPy6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts = list(lenta.title_and_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3jsY44oMmn-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_norm = [normalize(text) for text in texts]\n",
        "data_norm = [text for text in data_norm if text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34ngxQSVM6LJ",
        "colab_type": "code",
        "outputId": "92cc422a-a0cb-4a3f-86c2-b3019b5d4f43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "data_norm[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['назвать регион россия самый высокий смертность рак вице-премьер социальный вопрос татьяна голиков рассказать какой регион россия зафиксировать наиболее высокий смертность рак сообщать риа новость слово голиков чаща онкологический заболевание становиться причина смерть псковский тверская тульский орловский область также севастополь вице-премьер напомнить главное фактор смертность россия рак болезнь система кровообращение начало год стать известно смертность онкологический заболевание среди россиянин снизиться впервые год данные росстат 2017 год рак умереть 289 тысяча человек это 3,5 процент маленький год ранее',\n",
              " 'австрия представить доказательство вина российский биатлонист австрийский правоохранительный орган представить доказательство нарушение российский биатлонист антидопинговый правило сообщить посол россия вена дмитрий любинский итог встреча уполномоченный адвокат дипмиссия представитель прокуратура страна передавать тасс действовать презумпция невиновность какой-либо ограничение свобода передвижение команда добавить посольство международный союз биатлонист ibu также применять санкция российский биатлонист продолжить выступление кубок мир полиция нагрянуть отель сборный россия хохфильцен вечером 12 декабрь написать биатлонист александр логинов считать виновный махинация переливание кровь биатлонист антон шипулина также попасть список полиция нанести отдельный визит тренироваться отдельно австрийский обертиллах обвинение спортсмен назвать бред также указать охота ведьма мировой биатлон австрия приём допинг уголовный преступление максимальный наказание употребление год тюрьма',\n",
              " 'обнаружить самый счастливый место планета сотрудник социальный сеть instagram проанализировать поставить пользователь смайлик геолокация хештег опубликовать итог 2018 год сообщаться официальный сайт instagram такой образ самый счастливый геолокация instagram признать диснейленд токио счастливый смайлик 2018 год пользователь ставить именно фотография японский диснейленд также эксперт назвать самый популярный фильтр лицо стать фильтр сердечко глаз например самый часто использовать хештег 2018 год metoo timesup marchforourlives ноябрь сотрудник британский ассоциация потребитель which составить рейтинг самый безопасный страна путешествие специалист проанализировать 20 самый популярный туристический направление четыре критерий уровень преступность угроза здоровье вероятность теракт стихийный бедствие самый безопасный страна весь параметр стать исландия',\n",
              " 'сша раскрыть сумма расход расследование российский дело начало расследование российский вмешательство выбор власть сша потратить 25 миллион доллар сообщать associated press ссылка отчёт министерство юстиция сша документ содержаться дать расход следствие апрель сентябрь 2018 год полгода потратить 4,6 миллион доллар который 3 миллион доллар уйти зарплата сотрудник 580 тысяча поездка сопутствующий расход ранее минюст сша публиковать отчёт затрата дело российский вмешательство предыдущий месяц 11 декабрь расследование спецпрокурор робер мюллер показать меньший мера 14 человек окружение президент сша дональд трамп контактировать россиянин время избирательный кампания последующий переходный период вступление должность глава государство мюллер 2017 год вести дело якобы российский вмешательство американский выбор 2016-метр поставить задача выяснить сговор штаб трамп россия кремль белый дом отвергать обвинение россия неоднократно обвинять вмешательство выбор президент сша помощь хакер июнь спецслужба выдвинуть заочный обвинение 12 российский разведчик данные спецслужба сша российский разведка использовать два хакерский группировка взлом сервер демократический партия',\n",
              " 'хакер рассказать план великобритания заминировать севастополь хакерский группировка anonymous опубликовать новое документ деятельность британский аналитический центр integrity initiative материал следовать центр получать финансирование некого институт государственный управление руководство кристофер доннелли хакер выложить открытый доступ паспорт резюме также сведение связь министерство оборона великобритания согласно документ 2014 год доннелли выдвинуть ряд предложение британский власть связь ситуация крым частность планировать заминировать севастопольский бухта окружить полуостров войско также уничтожить остаться крым самолёт знак серьёзность намерение публикация заметка часть документ который член группировка ссылаться свой расследование удалить также хакер утверждать доннелли инициатор расследование российский вмешательство референдум независимость каталония пригласить член испанский отделение integrity initiative качество свидетель данные группировка доннелли получать деньга это министерство иностранный дело великобритания британский разведка платить собственный агент фальшивый доказательство вмешательство россия каталонский референдум затем приказать солгать парламент цель убедить предпринять антироссийский шаг писать хакер член anonymous опубликовать часть документ связанный деятельность кристофер доннелли расследование великобритания отношение integrity initiative дать результат пригрозить выложить новое доказательство доннелли тесно взаимодействовать британский спецслужба использовать свой положение влияние политика страна 23 ноябрь anonymous впервые опубликовать документ integrity initiative который содержимый инструкция борьба российский пропаганда пример дезинформация сторона москва хакер утверждать проект финансироваться правительство великобритания работать сразу несколько страна число германия франция испания']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 228
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slJsnX0dNdDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w2v = gensim.models.Word2Vec([text.split() for text in data_norm], size=50, sg=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPpbMrF7Nipg",
        "colab_type": "code",
        "outputId": "ad35802a-e77d-47b3-d4f2-a94e82510976",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f0832305-696e-4872-b45d-6904ee5466db\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-f0832305-696e-4872-b45d-6904ee5466db\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving paraphrases.xml to paraphrases.xml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io4Gyy2xfNev",
        "colab_type": "code",
        "outputId": "c637c083-cb1e-4022-aa69-8d49fa40c88d",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-635ffc07-7460-4f19-9d96-e12393c986f0\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-635ffc07-7460-4f19-9d96-e12393c986f0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving paraphrases_no_head.xml to paraphrases_no_head.xml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjLgsm3kiPio",
        "colab_type": "code",
        "outputId": "39a16a14-020b-4a23-b664-6ab8a59ac4f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 96
        }
      },
      "source": [
        "tree = ET.ElementTree(file='paraphrases.xml')\n",
        "root = tree.getroot()\n",
        "for child_of_root in root:\n",
        "    print(child_of_root.tag, child_of_root.attrib)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParseError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32munknown\u001b[0m\n\u001b[0;31mParseError\u001b[0m\u001b[0;31m:\u001b[0m not well-formed (invalid token): line 15, column 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a712Bf0_imip",
        "colab_type": "text"
      },
      "source": [
        "Колаб ругается на плохо оформленный файл. Видимо, придётся извлекать текст регулярками. Почему подошёл индекс скобочной группы 2, а не 1, я не знаю"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lNJEYThiyX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "paraphrases_df = pd.DataFrame({'text1': [], 'text2': [], 'class': [], 'jaccard': []})\n",
        "with open('paraphrases.xml', 'r', encoding='utf=8') as f:\n",
        "   text = f.read()\n",
        "text = re.sub('Ё', 'Е', text) #затем будем использовать Ё как разделитель\n",
        "text = re.sub('</?paraphrase>', 'Ё', text)\n",
        "#print(text[:500])\n",
        "paraphrases = re.findall('Ё[^Ё]+Ё', text)\n",
        "for paraphrase_description in paraphrases:\n",
        "    text1 = re.search('(<value name=\"text_1\">)([^<]+)', paraphrase_description)\n",
        "    text1 = normalize(text1.group(2))\n",
        "    text2 = re.search('(<value name=\"text_2\">)([^<]+)', paraphrase_description)\n",
        "    text2 = normalize(text2.group(2))\n",
        "    textclass = re.search('(<value name=\"class\">)([^<]+)', paraphrase_description).group(2)\n",
        "    jaccard = re.search('(<value name=\"jaccard\">)([^<]+)', paraphrase_description).group(2)\n",
        "    jaccard = float(re.sub(' ', '', jaccard))\n",
        "    paraphrases_df.loc[len(paraphrases_df)+1] = [text1, text2, textclass, jaccard]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gu_BZ7Eaell-",
        "colab_type": "code",
        "outputId": "ffbc3544-187e-4c49-8d44-9a5f65e7531b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "paraphrases_df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text1</th>\n",
              "      <th>text2</th>\n",
              "      <th>class</th>\n",
              "      <th>jaccard</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>полицейский разрешить стрелять поражение гражд...</td>\n",
              "      <td>полиция мочь разрешить стрелять хулиган травма...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.650000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>право полицейский проникновение жилища решить ...</td>\n",
              "      <td>правило внесудебный проникновение полицейский ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>президент египет ввести чрезвычайный положение...</td>\n",
              "      <td>власть египет угрожать ввести страна чрезвычай...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.611429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>вернуться сирия россиянин волновать вопрос тру...</td>\n",
              "      <td>самолёт мчс вывезти россиянин разрушить сирия</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.324037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>москва сирия вернуться 2 самолёт мчс россиянин...</td>\n",
              "      <td>самолёт мчс вывезти россиянин разрушить сирия</td>\n",
              "      <td>0</td>\n",
              "      <td>0.606218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7223</th>\n",
              "      <td>путин освободить должность ряд генералов</td>\n",
              "      <td>путин снять должность 20 руководитель-силовик</td>\n",
              "      <td>0</td>\n",
              "      <td>0.462500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7224</th>\n",
              "      <td>облако москва день победа разогнать девять сам...</td>\n",
              "      <td>путеводитель день победа провести 9 май москва</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.457143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7225</th>\n",
              "      <td>люблян отпраздновать день победа вместе москва</td>\n",
              "      <td>москва ограничить движение связь днём победа</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.584237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7226</th>\n",
              "      <td>девять самолёт ввс разогнать облако москва ден...</td>\n",
              "      <td>москва ограничить движение связь днём победа</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.461880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7227</th>\n",
              "      <td>9 май метрополитен петербург работать круглосу...</td>\n",
              "      <td>мартынов комендантский час донецк 9 май отменный</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.461880</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7227 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text1  ...   jaccard\n",
              "1     полицейский разрешить стрелять поражение гражд...  ...  0.650000\n",
              "2     право полицейский проникновение жилища решить ...  ...  0.500000\n",
              "3     президент египет ввести чрезвычайный положение...  ...  0.611429\n",
              "4     вернуться сирия россиянин волновать вопрос тру...  ...  0.324037\n",
              "5     москва сирия вернуться 2 самолёт мчс россиянин...  ...  0.606218\n",
              "...                                                 ...  ...       ...\n",
              "7223           путин освободить должность ряд генералов  ...  0.462500\n",
              "7224  облако москва день победа разогнать девять сам...  ...  0.457143\n",
              "7225     люблян отпраздновать день победа вместе москва  ...  0.584237\n",
              "7226  девять самолёт ввс разогнать облако москва ден...  ...  0.461880\n",
              "7227  9 май метрополитен петербург работать круглосу...  ...  0.461880\n",
              "\n",
              "[7227 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6KVZFTIlNLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "paraphrases_df.to_csv('paraphrases_lemmas_no_tags.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pbt9ly1lcAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('paraphrases_lemmas_no_tags.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmBj1htHawDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text2vector(text):\n",
        "  text = normalize(text)\n",
        "  text = text.split()\n",
        "  textvectors = []\n",
        "  for word in text:\n",
        "      try:\n",
        "          textvectors.append(w2v[word])\n",
        "      except KeyError:\n",
        "          pass\n",
        "  if textvectors:\n",
        "      return [np.sum(textvectors, axis=0), True]\n",
        "  else:\n",
        "      return [np.zeros(50), False] #теоретически может случайно получиться сумма ноль, поэтому маркер вычисления вектора вынесла в отдельную переменную"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28uenLCrWqU8",
        "colab_type": "code",
        "outputId": "4b87e86e-785b-4d0a-a67a-a6fc5aa5e765",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qU2iHg9RNJO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "first_texts = list(paraphrases_df.text1)\n",
        "second_texts = list(paraphrases_df.text2)\n",
        "textclass = list(paraphrases_df['class'])\n",
        "all_feaches = []\n",
        "real_classes = []\n",
        "for i in range(len(first_texts)):\n",
        "    vector1 = text2vector(first_texts[i])\n",
        "    vector2 = text2vector(second_texts[i])\n",
        "    if vector1[1] and vector2[1]:\n",
        "        all_feaches.append(np.concatenate([vector1[0], vector2[0]]))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RWzVL9GE8hu",
        "colab_type": "code",
        "outputId": "49adfa3f-aa3d-4445-d3c9-031fa0faed71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "source": [
        "my_vectors_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>vectors</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[-1.3110082, -0.24897516, -0.34025034, -1.9920...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[-1.1590409, -0.09042022, 0.0381108, -1.717871...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[-0.9237567, 0.8953589, 0.12389532, -2.8526745...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[-1.2208245, 0.32234126, 0.23125248, -2.145326...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[-1.9691132, 0.23433006, -0.4408493, -3.371620...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7184</th>\n",
              "      <td>[-0.7608899, 0.9300206, 0.33381748, -1.9772131...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7185</th>\n",
              "      <td>[-1.0949445, 0.051020436, -0.53552425, -2.1961...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7186</th>\n",
              "      <td>[-0.9251123, 0.30018207, -0.3646065, -1.723035...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7187</th>\n",
              "      <td>[-1.0949445, 0.051020443, -0.53552425, -2.1961...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7188</th>\n",
              "      <td>[-1.2081368, 0.15622276, -0.96440935, -2.10958...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7189 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                vectors class\n",
              "0     [-1.3110082, -0.24897516, -0.34025034, -1.9920...     0\n",
              "1     [-1.1590409, -0.09042022, 0.0381108, -1.717871...     0\n",
              "2     [-0.9237567, 0.8953589, 0.12389532, -2.8526745...     0\n",
              "3     [-1.2208245, 0.32234126, 0.23125248, -2.145326...    -1\n",
              "4     [-1.9691132, 0.23433006, -0.4408493, -3.371620...     0\n",
              "...                                                 ...   ...\n",
              "7184  [-0.7608899, 0.9300206, 0.33381748, -1.9772131...     0\n",
              "7185  [-1.0949445, 0.051020436, -0.53552425, -2.1961...    -1\n",
              "7186  [-0.9251123, 0.30018207, -0.3646065, -1.723035...    -1\n",
              "7187  [-1.0949445, 0.051020443, -0.53552425, -2.1961...    -1\n",
              "7188  [-1.2081368, 0.15622276, -0.96440935, -2.10958...    -1\n",
              "\n",
              "[7189 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-Erp0nwIYFq",
        "colab_type": "code",
        "outputId": "ff801e4b-1303-46fc-d185-5c2494968932",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(my_vectors_data.vectors[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWoYgAFTTASk",
        "colab_type": "code",
        "outputId": "66c4be1e-a753-4a80-8ea7-0bb8f7020aff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "feaches = list(my_vectors_data.vectors)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-167-ce0afe3287ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeaches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_vectors_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'my_vectors_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_of4-WuuFpq0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_df_from_vectors(vectors, len_vector=100):\n",
        "  dict_for_df = {}\n",
        "  for i in range(len_vector):\n",
        "    dict_for_df[i] = []\n",
        "  my_vector_feaches = pd.DataFrame(dict_for_df)\n",
        "  for vector in vectors:\n",
        "      my_vector_feaches.loc[len(my_vector_feaches)] = vector\n",
        "  return my_vector_feaches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_CgLkJ3Ow2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feaches_df.to_csv('my_existed_vectors.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-zBhNH5QFAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Zq5SCkXQOkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('my_existed_vectors.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ng7a1Xk1Qg8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_vectors_data.to_csv('my_existed_data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg99pd0vRlkY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('my_existed_data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukf-0u2rI4v-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = feaches_df\n",
        "y = my_vectors_data['class']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X2oGMPRac14",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hbje3Ne5a1Z7",
        "colab_type": "code",
        "outputId": "eca8e2a7-c414-41bc-ec77-665b9accbd00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression().fit(X_train, y_train)\n",
        "predicted = clf.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_HgjtKZJ8va",
        "colab_type": "code",
        "outputId": "278ab329-b7fa-4780-d64e-d2a63628754e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "print(f1_score(y_test, predicted, average='micro'))\n",
        "print(f1_score(y_test, predicted, average='macro'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.43882091212458285\n",
            "0.3308714620119157\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp_KOzE7Kz0R",
        "colab_type": "text"
      },
      "source": [
        "Не похоже на что-то вразумительное (впрочем, не удивительно при маленьком корпусе w2v).\n",
        "Сделаем кросс-валидацию (кросс-валидация для всех пунктов задания - в конце тетрадки)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jec3AraZXsoN",
        "colab_type": "text"
      },
      "source": [
        "На примере полученных векторов посмотрим, как считать косинусное расстояние"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDkT4_1IMjzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn\n",
        "def get_cos_distance(text1, text2):\n",
        "    text1 = normalize(text1)\n",
        "    text2 = normalize(text2)\n",
        "    text1 = text1.split()\n",
        "    text2 = text2.split()\n",
        "    text1vectors = []\n",
        "    text2vectors = []\n",
        "    text1representation = np.zeros(50)\n",
        "    text2representation = np.zeros(50)\n",
        "    not_empty1 = False\n",
        "    not_empty2 = False\n",
        "    text2vectors = []\n",
        "    for word in text1:\n",
        "        try:\n",
        "            text1vectors.append(w2v[word])\n",
        "            not_empty1 = True\n",
        "        except KeyError:\n",
        "            pass\n",
        "    for word in text2:\n",
        "        try:\n",
        "            text2vectors.append(w2v[word])\n",
        "            not_empty2 = True\n",
        "        except KeyError:\n",
        "            pass\n",
        "  #print(text1vectors)\n",
        "  #print(text2vectors)\n",
        "    if not_empty1 and not_empty2:\n",
        "        text1vectors = np.array(text1vectors)\n",
        "        text12ectors = np.array(text2vectors)\n",
        "        text1representation = np.sum(text1vectors, axis = 0)\n",
        "        text2representation = np.sum(text2vectors, axis = 0)\n",
        "        #print(text1representation)\n",
        "        cos =  sklearn.metrics.pairwise.cosine_distances([text1representation, text2representation]\n",
        "        return cos\n",
        "    else:\n",
        "        return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAtWn9TQbpcT",
        "colab_type": "code",
        "outputId": "23f77bcc-7127-4d3c-b50f-35e2ecc76eba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "dummy_example = get_cos_distance('Путин и кошка посетили США, Россию и зоопарк', 'В России запретили заводить кошек и попугаев')\n",
        "print(dummy_example)\n",
        "print(sum(dummy_example[0]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.03768897]\n",
            " [0.03768897 0.        ]]\n",
            "0.0376889705657959\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eOUeZyehZJqv",
        "colab": {}
      },
      "source": [
        "import sklearn\n",
        "def get_cos_distance(text1, text2):\n",
        "    text1 = normalize(text1)\n",
        "    text2 = normalize(text2)\n",
        "    text1 = text1.split()\n",
        "    text2 = text2.split()\n",
        "    text1vectors = []\n",
        "    text2vectors = []\n",
        "    text1representation = np.zeros(50)\n",
        "    text2representation = np.zeros(50)\n",
        "    not_empty1 = False\n",
        "    not_empty2 = False\n",
        "    text2vectors = []\n",
        "    for word in text1:\n",
        "        try:\n",
        "            text1vectors.append(w2v[word])\n",
        "            not_empty1 = True\n",
        "        except KeyError:\n",
        "            pass\n",
        "    for word in text2:\n",
        "        try:\n",
        "            text2vectors.append(w2v[word])\n",
        "            not_empty2 = True\n",
        "        except KeyError:\n",
        "            pass\n",
        "    if not_empty1 and not_empty2:\n",
        "        text1vectors = np.array(text1vectors)\n",
        "        text12ectors = np.array(text2vectors)\n",
        "        text1representation = np.sum(text1vectors, axis = 0)\n",
        "        text2representation = np.sum(text2vectors, axis = 0)\n",
        "        cos =  sklearn.metrics.pairwise.cosine_distances([text1representation, text2representation])\n",
        "        return sum(cos[0])\n",
        "    else:\n",
        "        return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rrJJQGQWU6-",
        "colab_type": "text"
      },
      "source": [
        "Какой-то странный формат результата хочется верить, что это результат сравнения векторов дважды с соседом и дважды с самими собой (где нули). Поэтому во втором варианте функции буду возвращать сумму первого элемента cos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufasZS5cY2lQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqPH1KwL7BlZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_for_regression = []\n",
        "first_texts = list(paraphrases_df.text1)\n",
        "second_texts = list(paraphrases_df.text2)\n",
        "class_data = list(paraphrases_df['class'])\n",
        "print()\n",
        "for i in range(len(first_texts)):\n",
        "    cos = get_cos_distance(first_texts[i], second_texts[i])\n",
        "    if cos:\n",
        "      data_for_regression.append([cos, class_data[i]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoOIWtlDH2yz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_for_regression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYxcPWAjHKKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cos = [i[0] for i in data_for_regression]\n",
        "text_class = [i[1] for i in data_for_regression]\n",
        "my_vectors_data = pd.DataFrame({'cos': cos, 'text_class':text_class})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyJv5P9me2LM",
        "colab_type": "code",
        "outputId": "a51d8469-ced4-48d7-fa98-98b7c2805ddd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "source": [
        "my_vectors_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cos</th>\n",
              "      <th>text_class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.010105</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.040688</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.020097</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.020606</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.020729</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7031</th>\n",
              "      <td>0.054010</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7032</th>\n",
              "      <td>0.034439</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7033</th>\n",
              "      <td>0.029383</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7034</th>\n",
              "      <td>0.052708</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7035</th>\n",
              "      <td>0.004931</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7036 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           cos text_class\n",
              "0     0.010105          0\n",
              "1     0.040688          0\n",
              "2     0.020097          0\n",
              "3     0.020606         -1\n",
              "4     0.020729          0\n",
              "...        ...        ...\n",
              "7031  0.054010          0\n",
              "7032  0.034439         -1\n",
              "7033  0.029383         -1\n",
              "7034  0.052708         -1\n",
              "7035  0.004931         -1\n",
              "\n",
              "[7036 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kp2mAaBe8OX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = my_vectors_data['cos'].to_numpy()\n",
        "y = my_vectors_data['text_class'].to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "es_gv5DzmVmW",
        "colab_type": "code",
        "outputId": "7b12e2c3-e662-4be1-c973-4905f8a41c18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQXHkWlKhMKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEGmrdc-hX7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression().fit(X_train.reshape(-1, 1), y_train)\n",
        "predicted = clf.predict(X_test.reshape(-1, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iWSS_yBnj7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qzx8Y0xEm_XS",
        "colab_type": "code",
        "outputId": "4aaf64ad-48a3-4188-f496-fc9eea6b138c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "f1_score(predicted, y_test, average = 'micro')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.46844798180784536"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzWo80sCntME",
        "colab_type": "code",
        "outputId": "61074d69-def3-41a0-fb9d-a3f5e5fd5b72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "f1_score(predicted, y_test, average = 'macro')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.34022222479114644"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8W6zylen97Z",
        "colab_type": "text"
      },
      "source": [
        "Результаты ужасные - впрочем, я и не рассчитывала получить ничего хорошего на 1000 текстов ленты."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYNQTeofel-K",
        "colab_type": "text"
      },
      "source": [
        "Воспользуемся моделью rusvectores\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9SE4h3PCoop",
        "colab_type": "code",
        "outputId": "8564241b-907f-4057-81fa-95c08101b6c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "!wget http://vectors.nlpl.eu/repository/11/180.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-19 07:36:17--  http://vectors.nlpl.eu/repository/11/180.zip\n",
            "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.225\n",
            "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.225|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 484452285 (462M) [application/zip]\n",
            "Saving to: ‘180.zip’\n",
            "\n",
            "180.zip             100%[===================>] 462.01M  21.5MB/s    in 27s     \n",
            "\n",
            "2020-02-19 07:36:45 (16.9 MB/s) - ‘180.zip’ saved [484452285/484452285]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6Mma-cGI4z5",
        "colab_type": "code",
        "outputId": "dde551c9-6046-4140-c4cb-efc90aebe921",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import zipfile\n",
        "#model_url = 'http://vectors.nlpl.eu/repository/11/180.zip'\n",
        "#m = wget.download(model_url)\n",
        "#model_file = model_url.split('/')[-1]\n",
        "with zipfile.ZipFile('180.zip', 'r') as archive:\n",
        "    stream = archive.open('model.bin')\n",
        "    model = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2C-UDus-HZb",
        "colab_type": "code",
        "outputId": "39355e4d-de70-4081-b4b1-ce367410506a",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#у меня очень медленно работает mystem \n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-963574e6-063c-42eb-823a-9463759d8f89\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-963574e6-063c-42eb-823a-9463759d8f89\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving data_paraphraser_norm.csv to data_paraphraser_norm.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1ulvj40H3hW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "paraphraser_norm = pd.read_csv('data_paraphraser_norm.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7xtJXkKJD-L",
        "colab_type": "code",
        "outputId": "e17a36ab-3ec4-47db-f2c6-e770874415d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        }
      },
      "source": [
        "paraphraser_norm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text_1</th>\n",
              "      <th>text_2</th>\n",
              "      <th>text_1_norm</th>\n",
              "      <th>text_2_norm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Полицейским разрешат стрелять на поражение по ...</td>\n",
              "      <td>Полиции могут разрешить стрелять по хулиганам ...</td>\n",
              "      <td>полицейский_NOUN разрешать_VERB стрелять_VERB ...</td>\n",
              "      <td>полиция_NOUN мочь_VERB разрешать_VERB стрелять...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Право полицейских на проникновение в жилище ре...</td>\n",
              "      <td>Правила внесудебного проникновения полицейских...</td>\n",
              "      <td>право_ADV полицейский_NOUN на_ADP проникновени...</td>\n",
              "      <td>правило_NOUN внесудебный_ADJ проникновение_NOU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>Президент Египта ввел чрезвычайное положение в...</td>\n",
              "      <td>Власти Египта угрожают ввести в стране чрезвыч...</td>\n",
              "      <td>президент_NOUN египет_NOUN вводить_VERB чрезвы...</td>\n",
              "      <td>власть_NOUN египет_NOUN угрожать_VERB вводить_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1</td>\n",
              "      <td>Вернувшихся из Сирии россиян волнует вопрос тр...</td>\n",
              "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
              "      <td>вернуться_VERB из_ADP сирия_NOUN россиянин_NOU...</td>\n",
              "      <td>самолет_NOUN мчс_NOUN вывозить_VERB россиянин_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>В Москву из Сирии вернулись 2 самолета МЧС с р...</td>\n",
              "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
              "      <td>в_ADP москва_NOUN из_ADP сирия_NOUN вернуться_...</td>\n",
              "      <td>самолет_NOUN мчс_NOUN вывозить_VERB россиянин_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7222</th>\n",
              "      <td>0</td>\n",
              "      <td>Путин освободил от должности ряд генералов</td>\n",
              "      <td>Путин снял с должностей более 20 руководителей...</td>\n",
              "      <td>путин_NOUN освобождать_VERB от_ADP должность_N...</td>\n",
              "      <td>путин_NOUN снимать_VERB с_ADP должность_NOUN м...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7223</th>\n",
              "      <td>-1</td>\n",
              "      <td>Облака над Москвой в День Победы разгонят девя...</td>\n",
              "      <td>Путеводитель по Дню Победы: как провести 9 мая...</td>\n",
              "      <td>облако_NOUN над_ADP москва_NOUN в_ADP день_NOU...</td>\n",
              "      <td>путеводитель_NOUN по_ADP день_NOUN победа_NOUN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7224</th>\n",
              "      <td>-1</td>\n",
              "      <td>Любляна отпразднует День Победы вместе с Москвой</td>\n",
              "      <td>В Москве ограничат движение в связи с Днем Победы</td>\n",
              "      <td>любляна_NOUN отпраздновать_VERB день_NOUN побе...</td>\n",
              "      <td>в_ADP москва_NOUN ограничивать_VERB движение_N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7225</th>\n",
              "      <td>-1</td>\n",
              "      <td>Девять самолетов ВВС разгонят облака над Москв...</td>\n",
              "      <td>В Москве ограничат движение в связи с Днем Победы</td>\n",
              "      <td>девять_NUM самолет_NOUN ввс_NOUN разгонять_VER...</td>\n",
              "      <td>в_ADP москва_NOUN ограничивать_VERB движение_N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7226</th>\n",
              "      <td>-1</td>\n",
              "      <td>9 мая метрополитен Петербурга будет работать к...</td>\n",
              "      <td>Мартынов: комендантский час в Донецке 9 мая бу...</td>\n",
              "      <td>май_NOUN метрополитен_NOUN петербург_NOUN быть...</td>\n",
              "      <td>мартынов_NOUN комендантский_ADJ час_NOUN в_ADP...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7227 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      label  ...                                        text_2_norm\n",
              "0         0  ...  полиция_NOUN мочь_VERB разрешать_VERB стрелять...\n",
              "1         0  ...  правило_NOUN внесудебный_ADJ проникновение_NOU...\n",
              "2         0  ...  власть_NOUN египет_NOUN угрожать_VERB вводить_...\n",
              "3        -1  ...  самолет_NOUN мчс_NOUN вывозить_VERB россиянин_...\n",
              "4         0  ...  самолет_NOUN мчс_NOUN вывозить_VERB россиянин_...\n",
              "...     ...  ...                                                ...\n",
              "7222      0  ...  путин_NOUN снимать_VERB с_ADP должность_NOUN м...\n",
              "7223     -1  ...  путеводитель_NOUN по_ADP день_NOUN победа_NOUN...\n",
              "7224     -1  ...  в_ADP москва_NOUN ограничивать_VERB движение_N...\n",
              "7225     -1  ...  в_ADP москва_NOUN ограничивать_VERB движение_N...\n",
              "7226     -1  ...  мартынов_NOUN комендантский_ADJ час_NOUN в_ADP...\n",
              "\n",
              "[7227 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9uhOlF0kEQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text2vector_rusvectores(text):\n",
        "    text = text.split()\n",
        "    vectors = []\n",
        "    for word in text:\n",
        "        try:\n",
        "            vectors.append(w2v[word])\n",
        "        except KeyError:\n",
        "            pass\n",
        "    if vectors:\n",
        "         return np.sum(vectors, axis=0)\n",
        "    else:\n",
        "         return np.zeros(300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyx1nOvCl7s0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qa8oIsSTSiEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w2v = model\n",
        "vectors = []\n",
        "labels = []\n",
        "texts1 = list(paraphraser_norm['text_1_norm'])\n",
        "texts2 = list(paraphraser_norm['text_2_norm'])\n",
        "for text1, text2 in zip(texts1, texts2):\n",
        "    vector1 = text2vector_rusvectores(text1)\n",
        "    vector2 = text2vector_rusvectores(text2)\n",
        "    vector = np.concatenate([vector1, vector2])\n",
        "    vectors.append(vector)\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QghjEgNlmZuh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dict_for_vectors = {}\n",
        "for i in range(600):\n",
        "    dict_for_vectors[i] = []\n",
        "rusvectore_data = pd.DataFrame(dict_for_vectors)\n",
        "for i in range(len(vectors)):\n",
        "    rusvectore_data.loc[len(rusvectore_data)] = vectors[i]\n",
        "rusvectore_data_with_class = rusvectore_data\n",
        "rusvectore_data_with_class['class'] = paraphraser_norm.label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81QhmYzMe02Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rusvectore_data_with_class.to_csv('rusvectore_data_with_class.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTVmcrsxjvsY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('rusvectore_data_with_class.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOEjE_OSwooP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = rusvectore_data\n",
        "y = paraphraser_norm.label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6Ur9cq-xBOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZ85Jk0fyy_J",
        "colab_type": "code",
        "outputId": "07082a5a-ae7c-4681-be54-9cccb5bec339",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression().fit(X_train, y_train)\n",
        "predicted = clf.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDvjznjizI5S",
        "colab_type": "code",
        "outputId": "1ea4f4ae-1bde-4838-a7b5-e70efffb6443",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "print(f1_score(y_test, predicted, average='micro'))\n",
        "print(f1_score(y_test, predicted, average='macro'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.945213060320974\n",
            "0.9458796135516906\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "861kbGHjzkrT",
        "colab_type": "text"
      },
      "source": [
        "С помощью модели, обученной на корпусе большого объема, получилось  намного более высокое качество работы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQRYN_ShErfY",
        "colab_type": "text"
      },
      "source": [
        "ЧАСТЬ 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8KgNPhWf04o",
        "colab_type": "text"
      },
      "source": [
        "2) Преобразуйте тексты в векторы в каждой паре 5 методами  - SVD, NMF, Word2Vec (свой и  русвекторовский), Fastext. У вас должно получиться 5 пар векторов для каждой строчки в датасете. Между векторами каждой пары вычислите косинусную близость (получится 5 чисел для каждой пары)\n",
        "\n",
        "Для этой задачи нецелесообразно пропускать предложения, которым модель не нашла векторное представление, поэтому использовать колонки датасетов из предыдущей части задания не получится."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFOO2w7q3Gxr",
        "colab_type": "text"
      },
      "source": [
        "SVD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79ReleB841gT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def get_cos_distance(vector1, vector2):\n",
        "    cos = sklearn.metrics.pairwise.cosine_distances([vector1, vector2])\n",
        "    return sum(cos[0])\n",
        "    \n",
        "          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEJSvnZ93K6U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf = TfidfVectorizer(min_df=3, max_df=0.4, max_features=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TKIfXO7cz6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidfmodel = tfidf.fit(data_norm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGQ9KI3DdMDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svd = TruncatedSVD(200)\n",
        "\n",
        "X_text_1 = svd.fit_transform(tfidfmodel.transform(paraphrases_df['text1']))\n",
        "X_text_2 = svd.fit_transform(tfidfmodel.transform(paraphrases_df['text2']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJfxMZ_ud_PH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z-5FzFegaLG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts1 = list(paraphrases_df.text1)\n",
        "texts2 = list(paraphrases_df.text2)\n",
        "text_class = list(paraphrases_df['class'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNdYDAKWgcXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count_vectorizer_cos = [get_cos_distance(X_text_1[i], X_text_2[i]) for i in range(len(X_text_1))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrwLSiCwgqz6",
        "colab_type": "text"
      },
      "source": [
        "Мой w2v"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgk-Lx_RfH-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cos_distance_my_w2v(text1, text2):\n",
        "    text1 = normalize(text1)\n",
        "    text2 = normalize(text2)\n",
        "    text1 = text1.split()\n",
        "    text2 = text2.split()\n",
        "    text1vectors = []\n",
        "    text2vectors = []\n",
        "    not_empty1 = False\n",
        "    not_empty2 = False\n",
        "    text2vectors = []\n",
        "    for word in text1:\n",
        "        try:\n",
        "            text1vectors.append(w2v[word])\n",
        "            not_empty1 = True\n",
        "        except KeyError:\n",
        "            pass\n",
        "    for word in text2:\n",
        "        try:\n",
        "            text2vectors.append(w2v[word])\n",
        "            not_empty2 = True\n",
        "        except KeyError:\n",
        "            pass\n",
        "    if not_empty1 and not_empty2:\n",
        "        text1vectors = np.array(text1vectors)\n",
        "        text12ectors = np.array(text2vectors)\n",
        "        text1representation = np.sum(text1vectors, axis = 0)\n",
        "        text2representation = np.sum(text2vectors, axis = 0)\n",
        "        cos =  sklearn.metrics.pairwise.cosine_distances([text1representation, text2representation])\n",
        "        return sum(cos[0])\n",
        "    else:\n",
        "        return 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LE2n7BNafsw8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_w2v_cos = [get_cos_distance_my_w2v(texts1[i], texts2[i]) for i in range(len(texts1))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEtMjxFn2lQm",
        "colab_type": "text"
      },
      "source": [
        "italicized text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNZw4EGF2oca",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZbcCVeyghuS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##NMF\n",
        "cv = CountVectorizer(min_df=3, max_df=0.4, max_features=1000)\n",
        "X = cv.fit_transform(data_norm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVWmCGqohIRB",
        "colab_type": "code",
        "outputId": "77cee0ad-2f3a-486a-9ff1-80cde3831bc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "nmf = NMF(50)\n",
        "nmf.fit(X)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
              "    n_components=50, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
              "    verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEoBtDPWhnVY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_text_1_nmf = nmf.transform(tfidf.transform(paraphrases_df['text1']))\n",
        "X_text_2_nmf = nmf.transform(tfidf.transform(paraphrases_df['text2']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB8HCpfth1O6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cosNMF = [get_cos_distance(X_text_1_nmf[i], X_text_2_nmf[i]) for i in range(len(X_text_1_nmf))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8mVoucLJTiM",
        "colab_type": "text"
      },
      "source": [
        "Rusvectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBZQOqqfMVWY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cos_distance_w2v(text1, text2):\n",
        "    text1 = text1.split()\n",
        "    text2 = text2.split()\n",
        "    text1vectors = []\n",
        "    text2vectors = []\n",
        "    not_empty1 = False\n",
        "    not_empty2 = False\n",
        "    text2vectors = []\n",
        "    for word in text1:\n",
        "        try:\n",
        "            text1vectors.append(w2v[word])\n",
        "            not_empty1 = True\n",
        "        except KeyError:\n",
        "            pass\n",
        "    for word in text2:\n",
        "        try:\n",
        "            text2vectors.append(w2v[word])\n",
        "            not_empty2 = True\n",
        "        except KeyError:\n",
        "            pass\n",
        "    if not_empty1 and not_empty2:\n",
        "        text1vectors = np.array(text1vectors)\n",
        "        text12ectors = np.array(text2vectors)\n",
        "        text1representation = np.sum(text1vectors, axis = 0)\n",
        "        text2representation = np.sum(text2vectors, axis = 0)\n",
        "        cos =  sklearn.metrics.pairwise.cosine_distances([text1representation, text2representation])\n",
        "        return sum(cos[0])\n",
        "    else:\n",
        "        return 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvyIRdh7JYUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts1 = list(paraphraser_norm.text_1_norm)\n",
        "texts2 = list(paraphraser_norm.text_2_norm)\n",
        "w2v=model\n",
        "rusvector_cos = [get_cos_distance_w2v(texts1[i], texts2[i]) for i in range(len(texts1))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7-MV7rNiJzi",
        "colab_type": "text"
      },
      "source": [
        "Fasttext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXOoOerWiO3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fast_text = gensim.models.FastText([text.split() for text in data_norm], size=50, \n",
        "                                   min_n=4, max_n=8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-11zUrVrigIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getvector_fasttext(text):\n",
        "    text = text.split()\n",
        "    vectors = []\n",
        "    for word in text:\n",
        "        try:\n",
        "            vectors.append(fast_text[word])\n",
        "        except KeyError:\n",
        "            pass\n",
        "    if vectors:\n",
        "        return np.sum(np.array(vectors), axis=0)\n",
        "    else:\n",
        "        return np.zeros(50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDbjK0qkj0NI",
        "colab_type": "code",
        "outputId": "601eb090-9570-4df8-e7d5-b36a30eabde7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "fasttextvectors1 = [getvector_fasttext(texts1[i]) for i in range(len(texts1))]\n",
        "fasttextvectors2 = [getvector_fasttext(texts2[i]) for i in range(len(texts2))]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnrPI6FDkgZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fasttext_cos = [get_cos_distance(fasttextvectors1[i], fasttextvectors2[i]) for i in range(len(fasttextvectors1))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3A-NqzAnnNj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_cos = pd.DataFrame({'rusvector': rusvector_cos,'fasttext': fasttext_cos, 'my_w2v': my_w2v_cos, 'cv': count_vectorizer_cos,  'NMF': cosNMF, 'text_class': text_class})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krATHoMIqpBZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_cos.to_csv('all_cos.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhaEdl3_3_qX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('all_cos.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqseTdh1NKmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_cos = pd.read_csv('all_cos.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrulH6a9N3pA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = all_cos.drop('text_class', axis=1)\n",
        "y = all_cos.text_class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hgo1tmZrPKcx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQHEQ7hqPy5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression().fit(X_train, y_train)\n",
        "predicted = clf.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LqpDxhm5plR",
        "colab_type": "code",
        "outputId": "93c83807-bc16-4cf7-8114-3ff928d71a25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(f1_score(y_test, predicted, average='micro'))\n",
        "print(f1_score(y_test, predicted, average='macro'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5589374654122855\n",
            "0.4769802906544473\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chOf1GT66ODC",
        "colab_type": "text"
      },
      "source": [
        "C пятью типами косинусных расстояний работает хуже, чем только с русвекторс без вычисления косинусных расстояний"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pazXEYlN6nqj",
        "colab_type": "text"
      },
      "source": [
        "Сдулаем кросс-валидацию для всех трёх задач с использованием уже полученных данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lklahDrChK8",
        "colab_type": "code",
        "outputId": "197095fb-bb20-4c8f-c81b-72324dd037d0",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "##w2v - моя модель\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-673efa53-8e9f-4121-8ec7-8960a2c19181\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-673efa53-8e9f-4121-8ec7-8960a2c19181\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving my_existed_data.csv to my_existed_data.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZRuA_N2EfdQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = pd.read_csv('my_existed_vectors.csv', index_col=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2RZGrgbiOfl",
        "colab_type": "code",
        "outputId": "26455eb1-7768-4855-f445-804fa0e0a654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "X"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.311008</td>\n",
              "      <td>-0.248975</td>\n",
              "      <td>-0.340250</td>\n",
              "      <td>-1.992067</td>\n",
              "      <td>-0.139105</td>\n",
              "      <td>-0.032127</td>\n",
              "      <td>-0.076155</td>\n",
              "      <td>-0.239314</td>\n",
              "      <td>0.491036</td>\n",
              "      <td>-0.140604</td>\n",
              "      <td>-0.302940</td>\n",
              "      <td>-0.036335</td>\n",
              "      <td>-0.347080</td>\n",
              "      <td>0.907033</td>\n",
              "      <td>-0.049920</td>\n",
              "      <td>-0.439422</td>\n",
              "      <td>0.040510</td>\n",
              "      <td>-0.457198</td>\n",
              "      <td>-1.265820</td>\n",
              "      <td>-1.702954</td>\n",
              "      <td>0.494942</td>\n",
              "      <td>0.678699</td>\n",
              "      <td>-0.423003</td>\n",
              "      <td>-2.271011</td>\n",
              "      <td>0.003091</td>\n",
              "      <td>-0.712701</td>\n",
              "      <td>0.098896</td>\n",
              "      <td>0.851776</td>\n",
              "      <td>1.039470</td>\n",
              "      <td>0.516789</td>\n",
              "      <td>0.877060</td>\n",
              "      <td>-0.074973</td>\n",
              "      <td>-0.408402</td>\n",
              "      <td>-0.071862</td>\n",
              "      <td>-0.251796</td>\n",
              "      <td>-1.198694</td>\n",
              "      <td>-0.309328</td>\n",
              "      <td>-0.236817</td>\n",
              "      <td>-2.006337</td>\n",
              "      <td>-0.173055</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.348343</td>\n",
              "      <td>0.102520</td>\n",
              "      <td>-0.215911</td>\n",
              "      <td>0.660795</td>\n",
              "      <td>-0.010149</td>\n",
              "      <td>-0.417014</td>\n",
              "      <td>-0.151916</td>\n",
              "      <td>-0.329753</td>\n",
              "      <td>-1.092657</td>\n",
              "      <td>-1.558959</td>\n",
              "      <td>0.354194</td>\n",
              "      <td>0.354065</td>\n",
              "      <td>-0.152676</td>\n",
              "      <td>-1.972251</td>\n",
              "      <td>-0.070930</td>\n",
              "      <td>-0.568488</td>\n",
              "      <td>0.170950</td>\n",
              "      <td>0.707940</td>\n",
              "      <td>0.899469</td>\n",
              "      <td>0.479140</td>\n",
              "      <td>0.431702</td>\n",
              "      <td>-0.244024</td>\n",
              "      <td>-0.373593</td>\n",
              "      <td>0.041137</td>\n",
              "      <td>-0.223628</td>\n",
              "      <td>-0.916934</td>\n",
              "      <td>-0.238360</td>\n",
              "      <td>-0.185444</td>\n",
              "      <td>-1.384313</td>\n",
              "      <td>-0.009521</td>\n",
              "      <td>1.264448</td>\n",
              "      <td>-0.611470</td>\n",
              "      <td>0.106642</td>\n",
              "      <td>0.440201</td>\n",
              "      <td>2.565839</td>\n",
              "      <td>1.100877</td>\n",
              "      <td>0.672132</td>\n",
              "      <td>-0.581036</td>\n",
              "      <td>0.079438</td>\n",
              "      <td>-1.044594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.159041</td>\n",
              "      <td>-0.090420</td>\n",
              "      <td>0.038111</td>\n",
              "      <td>-1.717872</td>\n",
              "      <td>0.037172</td>\n",
              "      <td>0.107198</td>\n",
              "      <td>0.021541</td>\n",
              "      <td>-0.321306</td>\n",
              "      <td>0.409480</td>\n",
              "      <td>-0.275182</td>\n",
              "      <td>-0.016309</td>\n",
              "      <td>0.278352</td>\n",
              "      <td>-0.163627</td>\n",
              "      <td>0.820600</td>\n",
              "      <td>0.090212</td>\n",
              "      <td>-0.394626</td>\n",
              "      <td>-0.125541</td>\n",
              "      <td>-0.340265</td>\n",
              "      <td>-1.170603</td>\n",
              "      <td>-1.353118</td>\n",
              "      <td>0.249228</td>\n",
              "      <td>0.171856</td>\n",
              "      <td>-0.312435</td>\n",
              "      <td>-1.882237</td>\n",
              "      <td>0.352919</td>\n",
              "      <td>-0.394006</td>\n",
              "      <td>0.223465</td>\n",
              "      <td>0.765786</td>\n",
              "      <td>1.304244</td>\n",
              "      <td>0.475647</td>\n",
              "      <td>0.337786</td>\n",
              "      <td>-0.296242</td>\n",
              "      <td>-0.372045</td>\n",
              "      <td>-0.104755</td>\n",
              "      <td>-0.254590</td>\n",
              "      <td>-1.031674</td>\n",
              "      <td>-0.342362</td>\n",
              "      <td>-0.415843</td>\n",
              "      <td>-1.667593</td>\n",
              "      <td>-0.103916</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.516263</td>\n",
              "      <td>-0.040716</td>\n",
              "      <td>-0.143097</td>\n",
              "      <td>0.532014</td>\n",
              "      <td>0.063996</td>\n",
              "      <td>-0.140910</td>\n",
              "      <td>-0.256549</td>\n",
              "      <td>-0.191705</td>\n",
              "      <td>-0.642446</td>\n",
              "      <td>-1.053517</td>\n",
              "      <td>0.394413</td>\n",
              "      <td>0.328395</td>\n",
              "      <td>-0.057132</td>\n",
              "      <td>-1.400808</td>\n",
              "      <td>0.082773</td>\n",
              "      <td>-0.351152</td>\n",
              "      <td>0.291511</td>\n",
              "      <td>0.309202</td>\n",
              "      <td>0.811218</td>\n",
              "      <td>0.361983</td>\n",
              "      <td>0.039432</td>\n",
              "      <td>-0.088947</td>\n",
              "      <td>-0.463851</td>\n",
              "      <td>-0.056303</td>\n",
              "      <td>-0.196005</td>\n",
              "      <td>-0.639110</td>\n",
              "      <td>-0.147295</td>\n",
              "      <td>-0.332103</td>\n",
              "      <td>-1.372345</td>\n",
              "      <td>0.015282</td>\n",
              "      <td>1.128340</td>\n",
              "      <td>-0.440991</td>\n",
              "      <td>0.129393</td>\n",
              "      <td>0.333615</td>\n",
              "      <td>1.958473</td>\n",
              "      <td>0.773750</td>\n",
              "      <td>0.559938</td>\n",
              "      <td>-0.450290</td>\n",
              "      <td>0.021824</td>\n",
              "      <td>-0.975151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.923757</td>\n",
              "      <td>0.895359</td>\n",
              "      <td>0.123895</td>\n",
              "      <td>-2.852675</td>\n",
              "      <td>0.404712</td>\n",
              "      <td>0.118081</td>\n",
              "      <td>0.094719</td>\n",
              "      <td>-1.940471</td>\n",
              "      <td>0.722107</td>\n",
              "      <td>-0.920715</td>\n",
              "      <td>1.399936</td>\n",
              "      <td>0.659026</td>\n",
              "      <td>-0.210487</td>\n",
              "      <td>1.923994</td>\n",
              "      <td>-0.954894</td>\n",
              "      <td>0.602918</td>\n",
              "      <td>0.585354</td>\n",
              "      <td>-0.796176</td>\n",
              "      <td>-1.654751</td>\n",
              "      <td>-1.299015</td>\n",
              "      <td>0.929309</td>\n",
              "      <td>-0.138684</td>\n",
              "      <td>-2.510516</td>\n",
              "      <td>-1.784721</td>\n",
              "      <td>1.308928</td>\n",
              "      <td>0.098066</td>\n",
              "      <td>0.763046</td>\n",
              "      <td>1.568040</td>\n",
              "      <td>1.832126</td>\n",
              "      <td>-0.751285</td>\n",
              "      <td>0.752463</td>\n",
              "      <td>0.049950</td>\n",
              "      <td>-0.268294</td>\n",
              "      <td>-0.276239</td>\n",
              "      <td>0.455596</td>\n",
              "      <td>-1.474053</td>\n",
              "      <td>0.517645</td>\n",
              "      <td>-0.607124</td>\n",
              "      <td>-2.559069</td>\n",
              "      <td>-0.125256</td>\n",
              "      <td>...</td>\n",
              "      <td>0.863260</td>\n",
              "      <td>0.043602</td>\n",
              "      <td>0.048752</td>\n",
              "      <td>2.166984</td>\n",
              "      <td>-0.800136</td>\n",
              "      <td>-0.148340</td>\n",
              "      <td>0.681162</td>\n",
              "      <td>-0.341793</td>\n",
              "      <td>-2.491975</td>\n",
              "      <td>-0.837232</td>\n",
              "      <td>1.174698</td>\n",
              "      <td>0.131322</td>\n",
              "      <td>-3.150895</td>\n",
              "      <td>-2.237692</td>\n",
              "      <td>2.561689</td>\n",
              "      <td>-0.005797</td>\n",
              "      <td>1.053265</td>\n",
              "      <td>2.048362</td>\n",
              "      <td>2.157361</td>\n",
              "      <td>-0.447366</td>\n",
              "      <td>0.874536</td>\n",
              "      <td>-0.114913</td>\n",
              "      <td>-0.553319</td>\n",
              "      <td>-0.314024</td>\n",
              "      <td>0.746305</td>\n",
              "      <td>-1.566579</td>\n",
              "      <td>-0.076725</td>\n",
              "      <td>-0.531591</td>\n",
              "      <td>-2.761251</td>\n",
              "      <td>0.052932</td>\n",
              "      <td>2.890112</td>\n",
              "      <td>-0.889786</td>\n",
              "      <td>1.006964</td>\n",
              "      <td>-0.371682</td>\n",
              "      <td>5.000759</td>\n",
              "      <td>2.263218</td>\n",
              "      <td>0.775530</td>\n",
              "      <td>0.598671</td>\n",
              "      <td>-0.598262</td>\n",
              "      <td>-2.223687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.220824</td>\n",
              "      <td>0.322341</td>\n",
              "      <td>0.231252</td>\n",
              "      <td>-2.145326</td>\n",
              "      <td>0.045807</td>\n",
              "      <td>-0.305271</td>\n",
              "      <td>0.274307</td>\n",
              "      <td>-0.698308</td>\n",
              "      <td>0.702603</td>\n",
              "      <td>-0.261407</td>\n",
              "      <td>0.513962</td>\n",
              "      <td>0.367395</td>\n",
              "      <td>-0.375328</td>\n",
              "      <td>1.056875</td>\n",
              "      <td>0.089443</td>\n",
              "      <td>-0.348706</td>\n",
              "      <td>0.192221</td>\n",
              "      <td>-0.388241</td>\n",
              "      <td>-1.367554</td>\n",
              "      <td>-1.620470</td>\n",
              "      <td>0.396031</td>\n",
              "      <td>0.065632</td>\n",
              "      <td>-0.870272</td>\n",
              "      <td>-1.992593</td>\n",
              "      <td>0.366420</td>\n",
              "      <td>-0.548009</td>\n",
              "      <td>-0.098619</td>\n",
              "      <td>1.137796</td>\n",
              "      <td>1.305375</td>\n",
              "      <td>0.107057</td>\n",
              "      <td>0.865487</td>\n",
              "      <td>-0.391812</td>\n",
              "      <td>-0.138522</td>\n",
              "      <td>-0.074607</td>\n",
              "      <td>0.097740</td>\n",
              "      <td>-1.130048</td>\n",
              "      <td>0.067654</td>\n",
              "      <td>-0.433988</td>\n",
              "      <td>-1.922678</td>\n",
              "      <td>-0.283224</td>\n",
              "      <td>...</td>\n",
              "      <td>0.025805</td>\n",
              "      <td>0.231074</td>\n",
              "      <td>-0.399917</td>\n",
              "      <td>0.860742</td>\n",
              "      <td>-0.051716</td>\n",
              "      <td>-0.584805</td>\n",
              "      <td>0.429397</td>\n",
              "      <td>-0.164500</td>\n",
              "      <td>-1.536881</td>\n",
              "      <td>-1.469716</td>\n",
              "      <td>0.480075</td>\n",
              "      <td>0.575745</td>\n",
              "      <td>-0.923857</td>\n",
              "      <td>-2.289916</td>\n",
              "      <td>0.301340</td>\n",
              "      <td>-0.776435</td>\n",
              "      <td>0.036652</td>\n",
              "      <td>1.119174</td>\n",
              "      <td>1.072621</td>\n",
              "      <td>0.052044</td>\n",
              "      <td>0.981223</td>\n",
              "      <td>-0.211071</td>\n",
              "      <td>-0.329867</td>\n",
              "      <td>-0.052104</td>\n",
              "      <td>0.062554</td>\n",
              "      <td>-0.966862</td>\n",
              "      <td>-0.155674</td>\n",
              "      <td>-0.165618</td>\n",
              "      <td>-1.853164</td>\n",
              "      <td>-0.181584</td>\n",
              "      <td>1.576429</td>\n",
              "      <td>-0.585031</td>\n",
              "      <td>0.519560</td>\n",
              "      <td>0.303875</td>\n",
              "      <td>2.875142</td>\n",
              "      <td>1.415654</td>\n",
              "      <td>0.993269</td>\n",
              "      <td>-0.376441</td>\n",
              "      <td>-0.075422</td>\n",
              "      <td>-1.401406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.969113</td>\n",
              "      <td>0.234330</td>\n",
              "      <td>-0.440849</td>\n",
              "      <td>-3.371621</td>\n",
              "      <td>0.080148</td>\n",
              "      <td>0.388621</td>\n",
              "      <td>-0.697308</td>\n",
              "      <td>-1.600268</td>\n",
              "      <td>1.085970</td>\n",
              "      <td>-0.313957</td>\n",
              "      <td>-0.157998</td>\n",
              "      <td>-0.054772</td>\n",
              "      <td>-0.815249</td>\n",
              "      <td>1.580745</td>\n",
              "      <td>-0.338655</td>\n",
              "      <td>-0.632400</td>\n",
              "      <td>0.817385</td>\n",
              "      <td>-0.274310</td>\n",
              "      <td>-2.137359</td>\n",
              "      <td>-1.895230</td>\n",
              "      <td>0.900337</td>\n",
              "      <td>1.378170</td>\n",
              "      <td>-1.275428</td>\n",
              "      <td>-3.598218</td>\n",
              "      <td>0.747741</td>\n",
              "      <td>-1.095244</td>\n",
              "      <td>0.467014</td>\n",
              "      <td>1.722107</td>\n",
              "      <td>1.749590</td>\n",
              "      <td>-0.053342</td>\n",
              "      <td>1.605013</td>\n",
              "      <td>-0.369478</td>\n",
              "      <td>-0.939280</td>\n",
              "      <td>-0.000579</td>\n",
              "      <td>0.227157</td>\n",
              "      <td>-1.326869</td>\n",
              "      <td>-0.021677</td>\n",
              "      <td>-0.119795</td>\n",
              "      <td>-3.706947</td>\n",
              "      <td>0.145125</td>\n",
              "      <td>...</td>\n",
              "      <td>0.025805</td>\n",
              "      <td>0.231074</td>\n",
              "      <td>-0.399917</td>\n",
              "      <td>0.860742</td>\n",
              "      <td>-0.051716</td>\n",
              "      <td>-0.584805</td>\n",
              "      <td>0.429397</td>\n",
              "      <td>-0.164500</td>\n",
              "      <td>-1.536881</td>\n",
              "      <td>-1.469716</td>\n",
              "      <td>0.480075</td>\n",
              "      <td>0.575745</td>\n",
              "      <td>-0.923857</td>\n",
              "      <td>-2.289916</td>\n",
              "      <td>0.301340</td>\n",
              "      <td>-0.776435</td>\n",
              "      <td>0.036652</td>\n",
              "      <td>1.119174</td>\n",
              "      <td>1.072621</td>\n",
              "      <td>0.052044</td>\n",
              "      <td>0.981223</td>\n",
              "      <td>-0.211071</td>\n",
              "      <td>-0.329867</td>\n",
              "      <td>-0.052104</td>\n",
              "      <td>0.062554</td>\n",
              "      <td>-0.966862</td>\n",
              "      <td>-0.155674</td>\n",
              "      <td>-0.165618</td>\n",
              "      <td>-1.853164</td>\n",
              "      <td>-0.181584</td>\n",
              "      <td>1.576429</td>\n",
              "      <td>-0.585031</td>\n",
              "      <td>0.519560</td>\n",
              "      <td>0.303875</td>\n",
              "      <td>2.875142</td>\n",
              "      <td>1.415654</td>\n",
              "      <td>0.993269</td>\n",
              "      <td>-0.376441</td>\n",
              "      <td>-0.075422</td>\n",
              "      <td>-1.401406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7184</th>\n",
              "      <td>-0.760890</td>\n",
              "      <td>0.930021</td>\n",
              "      <td>0.333817</td>\n",
              "      <td>-1.977213</td>\n",
              "      <td>0.275118</td>\n",
              "      <td>0.150149</td>\n",
              "      <td>0.343553</td>\n",
              "      <td>-1.152018</td>\n",
              "      <td>0.432856</td>\n",
              "      <td>-0.666037</td>\n",
              "      <td>1.107439</td>\n",
              "      <td>0.891713</td>\n",
              "      <td>-0.267115</td>\n",
              "      <td>1.123834</td>\n",
              "      <td>-0.048543</td>\n",
              "      <td>0.299947</td>\n",
              "      <td>0.252640</td>\n",
              "      <td>-0.444795</td>\n",
              "      <td>-0.978932</td>\n",
              "      <td>-1.260124</td>\n",
              "      <td>0.342253</td>\n",
              "      <td>-0.477633</td>\n",
              "      <td>-1.187038</td>\n",
              "      <td>-1.213541</td>\n",
              "      <td>0.754607</td>\n",
              "      <td>-0.016415</td>\n",
              "      <td>0.357184</td>\n",
              "      <td>1.048200</td>\n",
              "      <td>1.505995</td>\n",
              "      <td>-0.448767</td>\n",
              "      <td>0.211280</td>\n",
              "      <td>-0.449156</td>\n",
              "      <td>-0.135228</td>\n",
              "      <td>-0.202138</td>\n",
              "      <td>0.233838</td>\n",
              "      <td>-0.835896</td>\n",
              "      <td>0.362182</td>\n",
              "      <td>-0.782319</td>\n",
              "      <td>-1.749208</td>\n",
              "      <td>-0.200191</td>\n",
              "      <td>...</td>\n",
              "      <td>1.027205</td>\n",
              "      <td>0.554614</td>\n",
              "      <td>-0.413263</td>\n",
              "      <td>1.027777</td>\n",
              "      <td>-0.118628</td>\n",
              "      <td>0.189534</td>\n",
              "      <td>0.448678</td>\n",
              "      <td>-0.688927</td>\n",
              "      <td>-0.837428</td>\n",
              "      <td>-1.390921</td>\n",
              "      <td>0.274611</td>\n",
              "      <td>0.153156</td>\n",
              "      <td>-1.083807</td>\n",
              "      <td>-1.204593</td>\n",
              "      <td>0.138958</td>\n",
              "      <td>-0.410009</td>\n",
              "      <td>-0.172890</td>\n",
              "      <td>1.065126</td>\n",
              "      <td>0.784983</td>\n",
              "      <td>-0.144520</td>\n",
              "      <td>1.237997</td>\n",
              "      <td>-0.269131</td>\n",
              "      <td>0.043858</td>\n",
              "      <td>-0.190593</td>\n",
              "      <td>0.103409</td>\n",
              "      <td>-1.060484</td>\n",
              "      <td>0.360366</td>\n",
              "      <td>-0.472569</td>\n",
              "      <td>-1.899043</td>\n",
              "      <td>-0.256844</td>\n",
              "      <td>0.766192</td>\n",
              "      <td>-0.607667</td>\n",
              "      <td>0.327829</td>\n",
              "      <td>-0.155333</td>\n",
              "      <td>2.578252</td>\n",
              "      <td>1.839482</td>\n",
              "      <td>0.388779</td>\n",
              "      <td>-0.188677</td>\n",
              "      <td>-0.262152</td>\n",
              "      <td>-1.211061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7185</th>\n",
              "      <td>-1.094944</td>\n",
              "      <td>0.051020</td>\n",
              "      <td>-0.535524</td>\n",
              "      <td>-2.196100</td>\n",
              "      <td>0.348813</td>\n",
              "      <td>0.217891</td>\n",
              "      <td>-0.513685</td>\n",
              "      <td>-0.879088</td>\n",
              "      <td>0.534984</td>\n",
              "      <td>-0.336442</td>\n",
              "      <td>-0.061096</td>\n",
              "      <td>-0.149171</td>\n",
              "      <td>-0.561175</td>\n",
              "      <td>0.896382</td>\n",
              "      <td>-0.455129</td>\n",
              "      <td>-0.443786</td>\n",
              "      <td>0.587380</td>\n",
              "      <td>-0.445744</td>\n",
              "      <td>-1.354783</td>\n",
              "      <td>-1.387673</td>\n",
              "      <td>0.542878</td>\n",
              "      <td>1.008261</td>\n",
              "      <td>-0.652819</td>\n",
              "      <td>-2.492432</td>\n",
              "      <td>-0.169183</td>\n",
              "      <td>-0.775310</td>\n",
              "      <td>-0.053466</td>\n",
              "      <td>0.970472</td>\n",
              "      <td>0.837368</td>\n",
              "      <td>0.054525</td>\n",
              "      <td>1.608505</td>\n",
              "      <td>-0.107521</td>\n",
              "      <td>-0.382748</td>\n",
              "      <td>0.059570</td>\n",
              "      <td>0.269584</td>\n",
              "      <td>-0.813957</td>\n",
              "      <td>0.272243</td>\n",
              "      <td>0.056826</td>\n",
              "      <td>-2.014230</td>\n",
              "      <td>0.136188</td>\n",
              "      <td>...</td>\n",
              "      <td>0.273341</td>\n",
              "      <td>-0.520312</td>\n",
              "      <td>-0.570706</td>\n",
              "      <td>1.259571</td>\n",
              "      <td>-1.105130</td>\n",
              "      <td>0.178750</td>\n",
              "      <td>0.458198</td>\n",
              "      <td>-1.022342</td>\n",
              "      <td>-0.902982</td>\n",
              "      <td>-1.814085</td>\n",
              "      <td>0.943554</td>\n",
              "      <td>1.311452</td>\n",
              "      <td>-0.958468</td>\n",
              "      <td>-2.497294</td>\n",
              "      <td>-0.701420</td>\n",
              "      <td>-0.769858</td>\n",
              "      <td>-0.088227</td>\n",
              "      <td>0.939994</td>\n",
              "      <td>0.376251</td>\n",
              "      <td>0.132159</td>\n",
              "      <td>2.465739</td>\n",
              "      <td>0.261929</td>\n",
              "      <td>-0.191833</td>\n",
              "      <td>-0.052727</td>\n",
              "      <td>0.577147</td>\n",
              "      <td>-1.259268</td>\n",
              "      <td>0.947534</td>\n",
              "      <td>-0.011596</td>\n",
              "      <td>-2.585234</td>\n",
              "      <td>0.219939</td>\n",
              "      <td>1.910359</td>\n",
              "      <td>-0.782414</td>\n",
              "      <td>0.060094</td>\n",
              "      <td>0.178786</td>\n",
              "      <td>3.616052</td>\n",
              "      <td>2.457130</td>\n",
              "      <td>1.863305</td>\n",
              "      <td>-0.034651</td>\n",
              "      <td>0.418111</td>\n",
              "      <td>-1.965197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7186</th>\n",
              "      <td>-0.925112</td>\n",
              "      <td>0.300182</td>\n",
              "      <td>-0.364607</td>\n",
              "      <td>-1.723036</td>\n",
              "      <td>0.231996</td>\n",
              "      <td>0.078093</td>\n",
              "      <td>-0.238692</td>\n",
              "      <td>-0.824806</td>\n",
              "      <td>0.359218</td>\n",
              "      <td>-0.216905</td>\n",
              "      <td>0.150578</td>\n",
              "      <td>-0.142058</td>\n",
              "      <td>-0.430755</td>\n",
              "      <td>0.862000</td>\n",
              "      <td>-0.360803</td>\n",
              "      <td>-0.096011</td>\n",
              "      <td>0.282210</td>\n",
              "      <td>-0.477503</td>\n",
              "      <td>-0.833089</td>\n",
              "      <td>-1.092406</td>\n",
              "      <td>0.412392</td>\n",
              "      <td>0.615000</td>\n",
              "      <td>-0.424321</td>\n",
              "      <td>-1.795617</td>\n",
              "      <td>-0.023147</td>\n",
              "      <td>-0.410583</td>\n",
              "      <td>0.015333</td>\n",
              "      <td>0.729891</td>\n",
              "      <td>0.781677</td>\n",
              "      <td>0.124905</td>\n",
              "      <td>1.149512</td>\n",
              "      <td>-0.165019</td>\n",
              "      <td>-0.258819</td>\n",
              "      <td>0.016506</td>\n",
              "      <td>0.214157</td>\n",
              "      <td>-0.711464</td>\n",
              "      <td>0.330287</td>\n",
              "      <td>-0.119491</td>\n",
              "      <td>-1.767506</td>\n",
              "      <td>0.099419</td>\n",
              "      <td>...</td>\n",
              "      <td>0.446337</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>-0.534534</td>\n",
              "      <td>1.424449</td>\n",
              "      <td>-0.132459</td>\n",
              "      <td>-0.034232</td>\n",
              "      <td>0.380175</td>\n",
              "      <td>-0.400480</td>\n",
              "      <td>-1.413935</td>\n",
              "      <td>-1.475188</td>\n",
              "      <td>0.662742</td>\n",
              "      <td>0.641747</td>\n",
              "      <td>-1.025135</td>\n",
              "      <td>-2.272318</td>\n",
              "      <td>0.901224</td>\n",
              "      <td>-0.398696</td>\n",
              "      <td>0.497663</td>\n",
              "      <td>1.190742</td>\n",
              "      <td>1.438423</td>\n",
              "      <td>-0.145993</td>\n",
              "      <td>0.992286</td>\n",
              "      <td>-0.268487</td>\n",
              "      <td>-0.416462</td>\n",
              "      <td>-0.167922</td>\n",
              "      <td>0.252180</td>\n",
              "      <td>-1.015609</td>\n",
              "      <td>0.282777</td>\n",
              "      <td>-0.530239</td>\n",
              "      <td>-2.903835</td>\n",
              "      <td>0.113044</td>\n",
              "      <td>1.890304</td>\n",
              "      <td>-0.615978</td>\n",
              "      <td>0.689891</td>\n",
              "      <td>0.112010</td>\n",
              "      <td>3.888063</td>\n",
              "      <td>1.970569</td>\n",
              "      <td>0.933300</td>\n",
              "      <td>-0.123491</td>\n",
              "      <td>-0.010297</td>\n",
              "      <td>-1.876154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7187</th>\n",
              "      <td>-1.094944</td>\n",
              "      <td>0.051020</td>\n",
              "      <td>-0.535524</td>\n",
              "      <td>-2.196100</td>\n",
              "      <td>0.348813</td>\n",
              "      <td>0.217891</td>\n",
              "      <td>-0.513685</td>\n",
              "      <td>-0.879088</td>\n",
              "      <td>0.534984</td>\n",
              "      <td>-0.336442</td>\n",
              "      <td>-0.061096</td>\n",
              "      <td>-0.149171</td>\n",
              "      <td>-0.561175</td>\n",
              "      <td>0.896382</td>\n",
              "      <td>-0.455129</td>\n",
              "      <td>-0.443786</td>\n",
              "      <td>0.587380</td>\n",
              "      <td>-0.445744</td>\n",
              "      <td>-1.354783</td>\n",
              "      <td>-1.387673</td>\n",
              "      <td>0.542878</td>\n",
              "      <td>1.008261</td>\n",
              "      <td>-0.652819</td>\n",
              "      <td>-2.492432</td>\n",
              "      <td>-0.169183</td>\n",
              "      <td>-0.775310</td>\n",
              "      <td>-0.053466</td>\n",
              "      <td>0.970472</td>\n",
              "      <td>0.837368</td>\n",
              "      <td>0.054525</td>\n",
              "      <td>1.608505</td>\n",
              "      <td>-0.107521</td>\n",
              "      <td>-0.382748</td>\n",
              "      <td>0.059570</td>\n",
              "      <td>0.269584</td>\n",
              "      <td>-0.813957</td>\n",
              "      <td>0.272243</td>\n",
              "      <td>0.056826</td>\n",
              "      <td>-2.014230</td>\n",
              "      <td>0.136188</td>\n",
              "      <td>...</td>\n",
              "      <td>0.446337</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>-0.534534</td>\n",
              "      <td>1.424449</td>\n",
              "      <td>-0.132459</td>\n",
              "      <td>-0.034232</td>\n",
              "      <td>0.380175</td>\n",
              "      <td>-0.400480</td>\n",
              "      <td>-1.413935</td>\n",
              "      <td>-1.475188</td>\n",
              "      <td>0.662742</td>\n",
              "      <td>0.641747</td>\n",
              "      <td>-1.025135</td>\n",
              "      <td>-2.272318</td>\n",
              "      <td>0.901224</td>\n",
              "      <td>-0.398696</td>\n",
              "      <td>0.497663</td>\n",
              "      <td>1.190742</td>\n",
              "      <td>1.438423</td>\n",
              "      <td>-0.145993</td>\n",
              "      <td>0.992286</td>\n",
              "      <td>-0.268487</td>\n",
              "      <td>-0.416462</td>\n",
              "      <td>-0.167922</td>\n",
              "      <td>0.252180</td>\n",
              "      <td>-1.015609</td>\n",
              "      <td>0.282777</td>\n",
              "      <td>-0.530239</td>\n",
              "      <td>-2.903835</td>\n",
              "      <td>0.113044</td>\n",
              "      <td>1.890304</td>\n",
              "      <td>-0.615978</td>\n",
              "      <td>0.689891</td>\n",
              "      <td>0.112010</td>\n",
              "      <td>3.888063</td>\n",
              "      <td>1.970569</td>\n",
              "      <td>0.933300</td>\n",
              "      <td>-0.123491</td>\n",
              "      <td>-0.010297</td>\n",
              "      <td>-1.876154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7188</th>\n",
              "      <td>-1.208137</td>\n",
              "      <td>0.156223</td>\n",
              "      <td>-0.964409</td>\n",
              "      <td>-2.109582</td>\n",
              "      <td>0.033677</td>\n",
              "      <td>-0.083891</td>\n",
              "      <td>-0.058855</td>\n",
              "      <td>-0.479484</td>\n",
              "      <td>0.377702</td>\n",
              "      <td>-0.105899</td>\n",
              "      <td>0.185047</td>\n",
              "      <td>-0.249362</td>\n",
              "      <td>-0.318800</td>\n",
              "      <td>0.925269</td>\n",
              "      <td>-0.549558</td>\n",
              "      <td>-0.077334</td>\n",
              "      <td>0.263816</td>\n",
              "      <td>-0.682532</td>\n",
              "      <td>-0.878949</td>\n",
              "      <td>-1.674037</td>\n",
              "      <td>0.724348</td>\n",
              "      <td>0.824556</td>\n",
              "      <td>-0.859841</td>\n",
              "      <td>-1.898569</td>\n",
              "      <td>-0.380846</td>\n",
              "      <td>-0.763448</td>\n",
              "      <td>-0.017238</td>\n",
              "      <td>0.861624</td>\n",
              "      <td>0.411110</td>\n",
              "      <td>0.178734</td>\n",
              "      <td>1.628579</td>\n",
              "      <td>0.188355</td>\n",
              "      <td>-0.168229</td>\n",
              "      <td>-0.152678</td>\n",
              "      <td>0.258987</td>\n",
              "      <td>-1.070750</td>\n",
              "      <td>0.400561</td>\n",
              "      <td>-0.129608</td>\n",
              "      <td>-1.960873</td>\n",
              "      <td>-0.084114</td>\n",
              "      <td>...</td>\n",
              "      <td>0.083505</td>\n",
              "      <td>-0.339638</td>\n",
              "      <td>-0.248703</td>\n",
              "      <td>0.796394</td>\n",
              "      <td>-0.623342</td>\n",
              "      <td>0.035866</td>\n",
              "      <td>0.253742</td>\n",
              "      <td>-0.650452</td>\n",
              "      <td>-0.640388</td>\n",
              "      <td>-1.375605</td>\n",
              "      <td>0.626829</td>\n",
              "      <td>0.888401</td>\n",
              "      <td>-0.736455</td>\n",
              "      <td>-1.580556</td>\n",
              "      <td>-0.455815</td>\n",
              "      <td>-0.629243</td>\n",
              "      <td>-0.037728</td>\n",
              "      <td>0.698927</td>\n",
              "      <td>0.149094</td>\n",
              "      <td>0.211469</td>\n",
              "      <td>1.585359</td>\n",
              "      <td>0.297785</td>\n",
              "      <td>-0.141117</td>\n",
              "      <td>-0.087018</td>\n",
              "      <td>0.230186</td>\n",
              "      <td>-0.997494</td>\n",
              "      <td>0.425882</td>\n",
              "      <td>-0.018566</td>\n",
              "      <td>-1.642633</td>\n",
              "      <td>0.008319</td>\n",
              "      <td>1.245319</td>\n",
              "      <td>-0.818178</td>\n",
              "      <td>-0.018412</td>\n",
              "      <td>0.024656</td>\n",
              "      <td>2.397686</td>\n",
              "      <td>1.692758</td>\n",
              "      <td>1.094437</td>\n",
              "      <td>-0.200322</td>\n",
              "      <td>0.209130</td>\n",
              "      <td>-1.246005</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7189 rows × 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0         1         2  ...        97        98        99\n",
              "0    -1.311008 -0.248975 -0.340250  ... -0.581036  0.079438 -1.044594\n",
              "1    -1.159041 -0.090420  0.038111  ... -0.450290  0.021824 -0.975151\n",
              "2    -0.923757  0.895359  0.123895  ...  0.598671 -0.598262 -2.223687\n",
              "3    -1.220824  0.322341  0.231252  ... -0.376441 -0.075422 -1.401406\n",
              "4    -1.969113  0.234330 -0.440849  ... -0.376441 -0.075422 -1.401406\n",
              "...        ...       ...       ...  ...       ...       ...       ...\n",
              "7184 -0.760890  0.930021  0.333817  ... -0.188677 -0.262152 -1.211061\n",
              "7185 -1.094944  0.051020 -0.535524  ... -0.034651  0.418111 -1.965197\n",
              "7186 -0.925112  0.300182 -0.364607  ... -0.123491 -0.010297 -1.876154\n",
              "7187 -1.094944  0.051020 -0.535524  ... -0.123491 -0.010297 -1.876154\n",
              "7188 -1.208137  0.156223 -0.964409  ... -0.200322  0.209130 -1.246005\n",
              "\n",
              "[7189 rows x 100 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9G2vwG2ErEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = pd.read_csv('my_existed_data.csv')['class']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "we70DnlXE3OG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGULxBVUc7Lh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_fold = 10\n",
        "folds = KFold(n_splits=n_fold, shuffle=True, random_state=0)\n",
        "stratified_folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=0)\n",
        "repeated_folds = RepeatedStratifiedKFold(n_splits=n_fold, n_repeats=20, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud7MYYpudJLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cv_train_model(X, y, X_test, y_test, folds=folds, plot_feature_importance=False, model=None):\n",
        "    prediction = np.zeros(len(X_test))\n",
        "    valid_scores = []\n",
        "    feature_importance = pd.DataFrame()\n",
        "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n",
        "        X_train, X_valid = X[train_index], X[valid_index]\n",
        "        y_train, y_valid = y[train_index], y[valid_index]\n",
        "          \n",
        "\n",
        "        model.fit(X_train, y_train)      \n",
        "        y_pred_test = model.predict(X_test)\n",
        "        y_pred_valid = model.predict(X_valid)\n",
        "        valid_score = f1_score(y_valid, y_pred_valid, average='macro')\n",
        "        \n",
        "        valid_scores.append(valid_score)\n",
        "        prediction += y_pred_test  \n",
        "        \n",
        "    prediction /= (fold_n + 1)\n",
        "    test_score = f1_score(y_test, prediction, average='macro')\n",
        "    print('Cross Validation mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(valid_scores), np.std(valid_scores)))\n",
        "    print(\"Score on test data: {0:.4f}\".format(test_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRc7kUJwHE4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1 = sklearn.linear_model.LogisticRegression()\n",
        "model2 = sklearn.linear_model.LogisticRegression(C=0.5)\n",
        "model3 = sklearn.linear_model.LogisticRegression(C=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK523gWefpx6",
        "colab_type": "code",
        "outputId": "8a237cbc-d89d-4655-867d-cbd2f75f5cbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "X_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>349</th>\n",
              "      <td>-1.144555</td>\n",
              "      <td>-0.180738</td>\n",
              "      <td>-0.268660</td>\n",
              "      <td>-2.566048</td>\n",
              "      <td>0.239301</td>\n",
              "      <td>0.118868</td>\n",
              "      <td>0.033191</td>\n",
              "      <td>-0.979342</td>\n",
              "      <td>1.082604</td>\n",
              "      <td>0.285701</td>\n",
              "      <td>-0.883526</td>\n",
              "      <td>-0.411753</td>\n",
              "      <td>0.051552</td>\n",
              "      <td>1.860374</td>\n",
              "      <td>0.135277</td>\n",
              "      <td>0.101448</td>\n",
              "      <td>-0.660326</td>\n",
              "      <td>-0.071150</td>\n",
              "      <td>-0.768138</td>\n",
              "      <td>-1.218594</td>\n",
              "      <td>0.985678</td>\n",
              "      <td>0.184919</td>\n",
              "      <td>-1.307512</td>\n",
              "      <td>-2.331560</td>\n",
              "      <td>1.327911</td>\n",
              "      <td>-0.469251</td>\n",
              "      <td>0.627817</td>\n",
              "      <td>0.762036</td>\n",
              "      <td>2.132146</td>\n",
              "      <td>0.697464</td>\n",
              "      <td>-0.037661</td>\n",
              "      <td>-0.489269</td>\n",
              "      <td>-1.273287</td>\n",
              "      <td>-0.272486</td>\n",
              "      <td>0.105939</td>\n",
              "      <td>-1.374295</td>\n",
              "      <td>-0.459895</td>\n",
              "      <td>-1.145211</td>\n",
              "      <td>-3.880970</td>\n",
              "      <td>-0.334371</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.106154</td>\n",
              "      <td>-0.030037</td>\n",
              "      <td>-0.076714</td>\n",
              "      <td>0.135558</td>\n",
              "      <td>-0.022892</td>\n",
              "      <td>-0.027653</td>\n",
              "      <td>-0.023148</td>\n",
              "      <td>-0.087208</td>\n",
              "      <td>-0.233993</td>\n",
              "      <td>-0.418644</td>\n",
              "      <td>0.112655</td>\n",
              "      <td>0.173130</td>\n",
              "      <td>-0.080023</td>\n",
              "      <td>-0.456057</td>\n",
              "      <td>-0.023050</td>\n",
              "      <td>-0.147191</td>\n",
              "      <td>0.072668</td>\n",
              "      <td>0.143916</td>\n",
              "      <td>0.170367</td>\n",
              "      <td>0.081610</td>\n",
              "      <td>0.144568</td>\n",
              "      <td>0.053808</td>\n",
              "      <td>-0.069241</td>\n",
              "      <td>-0.020003</td>\n",
              "      <td>-0.061108</td>\n",
              "      <td>-0.236937</td>\n",
              "      <td>0.011350</td>\n",
              "      <td>-0.064710</td>\n",
              "      <td>-0.409159</td>\n",
              "      <td>0.013316</td>\n",
              "      <td>0.318123</td>\n",
              "      <td>-0.192023</td>\n",
              "      <td>0.010937</td>\n",
              "      <td>0.109890</td>\n",
              "      <td>0.630593</td>\n",
              "      <td>0.316636</td>\n",
              "      <td>0.201553</td>\n",
              "      <td>-0.152344</td>\n",
              "      <td>0.009660</td>\n",
              "      <td>-0.309653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>452</th>\n",
              "      <td>-1.102084</td>\n",
              "      <td>1.018204</td>\n",
              "      <td>-0.246298</td>\n",
              "      <td>-1.471660</td>\n",
              "      <td>-0.107958</td>\n",
              "      <td>0.083591</td>\n",
              "      <td>0.259912</td>\n",
              "      <td>-0.877863</td>\n",
              "      <td>0.075351</td>\n",
              "      <td>-0.238104</td>\n",
              "      <td>0.477495</td>\n",
              "      <td>0.150056</td>\n",
              "      <td>-0.177788</td>\n",
              "      <td>0.488618</td>\n",
              "      <td>0.137809</td>\n",
              "      <td>0.415481</td>\n",
              "      <td>0.064643</td>\n",
              "      <td>-0.268972</td>\n",
              "      <td>-0.614848</td>\n",
              "      <td>-1.409200</td>\n",
              "      <td>0.197740</td>\n",
              "      <td>0.065551</td>\n",
              "      <td>-0.469780</td>\n",
              "      <td>-0.731847</td>\n",
              "      <td>0.538576</td>\n",
              "      <td>0.076147</td>\n",
              "      <td>0.491378</td>\n",
              "      <td>0.376700</td>\n",
              "      <td>0.682918</td>\n",
              "      <td>-0.139392</td>\n",
              "      <td>-0.057107</td>\n",
              "      <td>0.229056</td>\n",
              "      <td>0.015588</td>\n",
              "      <td>-0.290822</td>\n",
              "      <td>0.055491</td>\n",
              "      <td>-0.713975</td>\n",
              "      <td>0.587531</td>\n",
              "      <td>-0.631412</td>\n",
              "      <td>-1.422124</td>\n",
              "      <td>0.034587</td>\n",
              "      <td>...</td>\n",
              "      <td>0.273571</td>\n",
              "      <td>0.154681</td>\n",
              "      <td>-0.199068</td>\n",
              "      <td>0.417274</td>\n",
              "      <td>0.089451</td>\n",
              "      <td>0.227728</td>\n",
              "      <td>0.031628</td>\n",
              "      <td>-0.379332</td>\n",
              "      <td>-0.715389</td>\n",
              "      <td>-1.576482</td>\n",
              "      <td>0.169109</td>\n",
              "      <td>0.082349</td>\n",
              "      <td>-0.249299</td>\n",
              "      <td>-0.940819</td>\n",
              "      <td>0.217243</td>\n",
              "      <td>-0.086407</td>\n",
              "      <td>0.321407</td>\n",
              "      <td>0.355188</td>\n",
              "      <td>0.621477</td>\n",
              "      <td>0.097231</td>\n",
              "      <td>0.064112</td>\n",
              "      <td>0.205673</td>\n",
              "      <td>0.037087</td>\n",
              "      <td>-0.220904</td>\n",
              "      <td>-0.125092</td>\n",
              "      <td>-0.760831</td>\n",
              "      <td>0.449403</td>\n",
              "      <td>-0.535637</td>\n",
              "      <td>-1.263109</td>\n",
              "      <td>-0.011951</td>\n",
              "      <td>0.591511</td>\n",
              "      <td>-0.778017</td>\n",
              "      <td>0.321372</td>\n",
              "      <td>0.229841</td>\n",
              "      <td>2.283345</td>\n",
              "      <td>0.969092</td>\n",
              "      <td>0.188738</td>\n",
              "      <td>-0.446956</td>\n",
              "      <td>-0.269786</td>\n",
              "      <td>-1.076639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>662</th>\n",
              "      <td>-1.147255</td>\n",
              "      <td>-0.089104</td>\n",
              "      <td>0.034505</td>\n",
              "      <td>-2.202101</td>\n",
              "      <td>-0.124775</td>\n",
              "      <td>-0.276407</td>\n",
              "      <td>-0.160643</td>\n",
              "      <td>-0.614947</td>\n",
              "      <td>0.851018</td>\n",
              "      <td>0.031052</td>\n",
              "      <td>0.137327</td>\n",
              "      <td>-0.009247</td>\n",
              "      <td>-0.492447</td>\n",
              "      <td>1.132982</td>\n",
              "      <td>-0.124578</td>\n",
              "      <td>-0.522341</td>\n",
              "      <td>0.362309</td>\n",
              "      <td>-0.353958</td>\n",
              "      <td>-1.464372</td>\n",
              "      <td>-1.501185</td>\n",
              "      <td>0.411067</td>\n",
              "      <td>0.577085</td>\n",
              "      <td>-0.725578</td>\n",
              "      <td>-2.236889</td>\n",
              "      <td>0.329789</td>\n",
              "      <td>-0.804006</td>\n",
              "      <td>-0.181487</td>\n",
              "      <td>1.260521</td>\n",
              "      <td>1.084073</td>\n",
              "      <td>0.276897</td>\n",
              "      <td>1.360959</td>\n",
              "      <td>-0.275105</td>\n",
              "      <td>-0.242335</td>\n",
              "      <td>0.027329</td>\n",
              "      <td>-0.114945</td>\n",
              "      <td>-1.143834</td>\n",
              "      <td>-0.218670</td>\n",
              "      <td>-0.107636</td>\n",
              "      <td>-2.243478</td>\n",
              "      <td>-0.183181</td>\n",
              "      <td>...</td>\n",
              "      <td>0.161495</td>\n",
              "      <td>-0.152588</td>\n",
              "      <td>-0.555188</td>\n",
              "      <td>1.256531</td>\n",
              "      <td>-0.237620</td>\n",
              "      <td>-0.481430</td>\n",
              "      <td>0.483060</td>\n",
              "      <td>-0.638902</td>\n",
              "      <td>-1.614513</td>\n",
              "      <td>-1.896666</td>\n",
              "      <td>0.578669</td>\n",
              "      <td>0.903972</td>\n",
              "      <td>-1.032915</td>\n",
              "      <td>-2.462727</td>\n",
              "      <td>0.078298</td>\n",
              "      <td>-1.031327</td>\n",
              "      <td>-0.329844</td>\n",
              "      <td>1.402067</td>\n",
              "      <td>0.861170</td>\n",
              "      <td>0.427934</td>\n",
              "      <td>1.927123</td>\n",
              "      <td>-0.047788</td>\n",
              "      <td>-0.147398</td>\n",
              "      <td>-0.000319</td>\n",
              "      <td>-0.104322</td>\n",
              "      <td>-1.492932</td>\n",
              "      <td>-0.124373</td>\n",
              "      <td>-0.076752</td>\n",
              "      <td>-2.499663</td>\n",
              "      <td>-0.287048</td>\n",
              "      <td>1.724922</td>\n",
              "      <td>-0.608680</td>\n",
              "      <td>0.327773</td>\n",
              "      <td>0.403088</td>\n",
              "      <td>3.191929</td>\n",
              "      <td>2.191225</td>\n",
              "      <td>1.002112</td>\n",
              "      <td>-0.509304</td>\n",
              "      <td>0.039833</td>\n",
              "      <td>-1.384262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>-0.810159</td>\n",
              "      <td>-0.112330</td>\n",
              "      <td>-0.514646</td>\n",
              "      <td>-1.184005</td>\n",
              "      <td>-0.121589</td>\n",
              "      <td>-0.006765</td>\n",
              "      <td>-0.093587</td>\n",
              "      <td>-0.154536</td>\n",
              "      <td>0.072264</td>\n",
              "      <td>-0.147635</td>\n",
              "      <td>-0.211876</td>\n",
              "      <td>-0.045221</td>\n",
              "      <td>-0.235910</td>\n",
              "      <td>0.467631</td>\n",
              "      <td>-0.182507</td>\n",
              "      <td>-0.144889</td>\n",
              "      <td>-0.010513</td>\n",
              "      <td>-0.279875</td>\n",
              "      <td>-0.724444</td>\n",
              "      <td>-1.215461</td>\n",
              "      <td>0.363585</td>\n",
              "      <td>0.548946</td>\n",
              "      <td>-0.259959</td>\n",
              "      <td>-1.359539</td>\n",
              "      <td>-0.143244</td>\n",
              "      <td>-0.445327</td>\n",
              "      <td>0.127943</td>\n",
              "      <td>0.470025</td>\n",
              "      <td>0.476528</td>\n",
              "      <td>0.255464</td>\n",
              "      <td>0.605511</td>\n",
              "      <td>0.204001</td>\n",
              "      <td>-0.183144</td>\n",
              "      <td>-0.058748</td>\n",
              "      <td>-0.211628</td>\n",
              "      <td>-0.727101</td>\n",
              "      <td>-0.003155</td>\n",
              "      <td>-0.102646</td>\n",
              "      <td>-1.186134</td>\n",
              "      <td>-0.014283</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.552027</td>\n",
              "      <td>-0.072107</td>\n",
              "      <td>-0.063127</td>\n",
              "      <td>0.462613</td>\n",
              "      <td>-0.377078</td>\n",
              "      <td>0.054003</td>\n",
              "      <td>-0.086344</td>\n",
              "      <td>-0.382862</td>\n",
              "      <td>-0.493716</td>\n",
              "      <td>-1.146756</td>\n",
              "      <td>0.610745</td>\n",
              "      <td>0.739140</td>\n",
              "      <td>-0.054400</td>\n",
              "      <td>-1.382437</td>\n",
              "      <td>-0.241103</td>\n",
              "      <td>-0.399143</td>\n",
              "      <td>0.439478</td>\n",
              "      <td>0.300165</td>\n",
              "      <td>0.474826</td>\n",
              "      <td>0.159964</td>\n",
              "      <td>0.576252</td>\n",
              "      <td>0.240867</td>\n",
              "      <td>-0.439512</td>\n",
              "      <td>0.021678</td>\n",
              "      <td>-0.136529</td>\n",
              "      <td>-0.658108</td>\n",
              "      <td>0.133634</td>\n",
              "      <td>-0.112442</td>\n",
              "      <td>-1.201622</td>\n",
              "      <td>0.200434</td>\n",
              "      <td>1.372716</td>\n",
              "      <td>-0.690241</td>\n",
              "      <td>-0.164011</td>\n",
              "      <td>0.060993</td>\n",
              "      <td>2.005353</td>\n",
              "      <td>1.119503</td>\n",
              "      <td>0.972258</td>\n",
              "      <td>-0.360762</td>\n",
              "      <td>0.422156</td>\n",
              "      <td>-1.071839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1405</th>\n",
              "      <td>-1.390972</td>\n",
              "      <td>0.343716</td>\n",
              "      <td>0.071179</td>\n",
              "      <td>-2.617934</td>\n",
              "      <td>-0.068685</td>\n",
              "      <td>-0.076096</td>\n",
              "      <td>0.088771</td>\n",
              "      <td>-0.955217</td>\n",
              "      <td>0.885980</td>\n",
              "      <td>-0.300754</td>\n",
              "      <td>0.417778</td>\n",
              "      <td>0.202701</td>\n",
              "      <td>-0.493528</td>\n",
              "      <td>1.275271</td>\n",
              "      <td>0.064648</td>\n",
              "      <td>-0.392855</td>\n",
              "      <td>0.294971</td>\n",
              "      <td>-0.313371</td>\n",
              "      <td>-1.643771</td>\n",
              "      <td>-1.745071</td>\n",
              "      <td>0.519154</td>\n",
              "      <td>0.430865</td>\n",
              "      <td>-1.017389</td>\n",
              "      <td>-2.266308</td>\n",
              "      <td>0.702524</td>\n",
              "      <td>-0.619495</td>\n",
              "      <td>0.173610</td>\n",
              "      <td>1.263540</td>\n",
              "      <td>1.408572</td>\n",
              "      <td>0.029532</td>\n",
              "      <td>0.990545</td>\n",
              "      <td>-0.240395</td>\n",
              "      <td>-0.335400</td>\n",
              "      <td>-0.163386</td>\n",
              "      <td>-0.037970</td>\n",
              "      <td>-1.203892</td>\n",
              "      <td>-0.092223</td>\n",
              "      <td>-0.377291</td>\n",
              "      <td>-2.445738</td>\n",
              "      <td>-0.219912</td>\n",
              "      <td>...</td>\n",
              "      <td>0.502197</td>\n",
              "      <td>0.240387</td>\n",
              "      <td>-0.405266</td>\n",
              "      <td>1.341871</td>\n",
              "      <td>0.163795</td>\n",
              "      <td>-0.306620</td>\n",
              "      <td>0.197980</td>\n",
              "      <td>-0.160533</td>\n",
              "      <td>-1.377116</td>\n",
              "      <td>-1.689829</td>\n",
              "      <td>0.543467</td>\n",
              "      <td>0.337276</td>\n",
              "      <td>-0.995367</td>\n",
              "      <td>-2.143998</td>\n",
              "      <td>0.996016</td>\n",
              "      <td>-0.424573</td>\n",
              "      <td>0.472627</td>\n",
              "      <td>1.229997</td>\n",
              "      <td>1.575285</td>\n",
              "      <td>-0.135116</td>\n",
              "      <td>0.645513</td>\n",
              "      <td>-0.375884</td>\n",
              "      <td>-0.463291</td>\n",
              "      <td>-0.314004</td>\n",
              "      <td>0.189981</td>\n",
              "      <td>-1.160682</td>\n",
              "      <td>-0.072535</td>\n",
              "      <td>-0.588261</td>\n",
              "      <td>-2.647754</td>\n",
              "      <td>-0.228656</td>\n",
              "      <td>1.622529</td>\n",
              "      <td>-0.976398</td>\n",
              "      <td>0.687539</td>\n",
              "      <td>-0.057373</td>\n",
              "      <td>3.751156</td>\n",
              "      <td>1.769526</td>\n",
              "      <td>0.545972</td>\n",
              "      <td>-0.376522</td>\n",
              "      <td>-0.228754</td>\n",
              "      <td>-1.782657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3772</th>\n",
              "      <td>-0.826017</td>\n",
              "      <td>0.447423</td>\n",
              "      <td>-0.087454</td>\n",
              "      <td>-1.286550</td>\n",
              "      <td>0.033891</td>\n",
              "      <td>0.080381</td>\n",
              "      <td>0.082979</td>\n",
              "      <td>-0.729203</td>\n",
              "      <td>0.242409</td>\n",
              "      <td>-0.226997</td>\n",
              "      <td>0.213809</td>\n",
              "      <td>0.092021</td>\n",
              "      <td>-0.188863</td>\n",
              "      <td>0.645388</td>\n",
              "      <td>0.003334</td>\n",
              "      <td>0.091931</td>\n",
              "      <td>0.003713</td>\n",
              "      <td>-0.137463</td>\n",
              "      <td>-0.647698</td>\n",
              "      <td>-0.977403</td>\n",
              "      <td>0.305364</td>\n",
              "      <td>0.059670</td>\n",
              "      <td>-0.592963</td>\n",
              "      <td>-1.085561</td>\n",
              "      <td>0.515055</td>\n",
              "      <td>-0.098513</td>\n",
              "      <td>0.355083</td>\n",
              "      <td>0.514790</td>\n",
              "      <td>0.825036</td>\n",
              "      <td>-0.097161</td>\n",
              "      <td>0.079610</td>\n",
              "      <td>-0.070199</td>\n",
              "      <td>-0.169442</td>\n",
              "      <td>-0.178780</td>\n",
              "      <td>0.121896</td>\n",
              "      <td>-0.558756</td>\n",
              "      <td>0.244673</td>\n",
              "      <td>-0.442575</td>\n",
              "      <td>-1.312756</td>\n",
              "      <td>-0.075497</td>\n",
              "      <td>...</td>\n",
              "      <td>0.096352</td>\n",
              "      <td>0.008602</td>\n",
              "      <td>-0.241667</td>\n",
              "      <td>0.567463</td>\n",
              "      <td>-0.011125</td>\n",
              "      <td>-0.194686</td>\n",
              "      <td>0.153068</td>\n",
              "      <td>-0.290509</td>\n",
              "      <td>-0.727368</td>\n",
              "      <td>-0.991743</td>\n",
              "      <td>0.261002</td>\n",
              "      <td>0.329226</td>\n",
              "      <td>-0.396393</td>\n",
              "      <td>-1.191565</td>\n",
              "      <td>0.063363</td>\n",
              "      <td>-0.423941</td>\n",
              "      <td>-0.054270</td>\n",
              "      <td>0.588818</td>\n",
              "      <td>0.529964</td>\n",
              "      <td>0.172831</td>\n",
              "      <td>0.675848</td>\n",
              "      <td>-0.030437</td>\n",
              "      <td>-0.111709</td>\n",
              "      <td>-0.044721</td>\n",
              "      <td>-0.049792</td>\n",
              "      <td>-0.694330</td>\n",
              "      <td>-0.003265</td>\n",
              "      <td>-0.164776</td>\n",
              "      <td>-1.225377</td>\n",
              "      <td>-0.134758</td>\n",
              "      <td>0.814098</td>\n",
              "      <td>-0.383769</td>\n",
              "      <td>0.203396</td>\n",
              "      <td>0.169289</td>\n",
              "      <td>1.669715</td>\n",
              "      <td>0.981859</td>\n",
              "      <td>0.429667</td>\n",
              "      <td>-0.294015</td>\n",
              "      <td>-0.053580</td>\n",
              "      <td>-0.734272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5191</th>\n",
              "      <td>-0.814944</td>\n",
              "      <td>0.310936</td>\n",
              "      <td>-0.501550</td>\n",
              "      <td>-2.542010</td>\n",
              "      <td>0.156315</td>\n",
              "      <td>-0.380839</td>\n",
              "      <td>0.198584</td>\n",
              "      <td>-0.603514</td>\n",
              "      <td>0.589579</td>\n",
              "      <td>0.375763</td>\n",
              "      <td>0.293741</td>\n",
              "      <td>-0.291019</td>\n",
              "      <td>0.031207</td>\n",
              "      <td>1.260243</td>\n",
              "      <td>-1.222025</td>\n",
              "      <td>-0.032522</td>\n",
              "      <td>0.698126</td>\n",
              "      <td>-0.548189</td>\n",
              "      <td>-0.720278</td>\n",
              "      <td>-1.489111</td>\n",
              "      <td>0.959456</td>\n",
              "      <td>0.781989</td>\n",
              "      <td>-1.626708</td>\n",
              "      <td>-1.662615</td>\n",
              "      <td>-0.598341</td>\n",
              "      <td>-0.977597</td>\n",
              "      <td>-0.445987</td>\n",
              "      <td>1.636928</td>\n",
              "      <td>0.655402</td>\n",
              "      <td>0.069429</td>\n",
              "      <td>2.567286</td>\n",
              "      <td>-0.167131</td>\n",
              "      <td>0.125716</td>\n",
              "      <td>-0.222662</td>\n",
              "      <td>0.722601</td>\n",
              "      <td>-1.440047</td>\n",
              "      <td>-0.172132</td>\n",
              "      <td>0.055430</td>\n",
              "      <td>-1.885864</td>\n",
              "      <td>-0.851791</td>\n",
              "      <td>...</td>\n",
              "      <td>1.811185</td>\n",
              "      <td>1.132169</td>\n",
              "      <td>-0.568562</td>\n",
              "      <td>1.714855</td>\n",
              "      <td>-0.454077</td>\n",
              "      <td>0.727896</td>\n",
              "      <td>0.338042</td>\n",
              "      <td>-1.025718</td>\n",
              "      <td>-1.015443</td>\n",
              "      <td>-1.808082</td>\n",
              "      <td>0.419242</td>\n",
              "      <td>-0.668088</td>\n",
              "      <td>-2.056448</td>\n",
              "      <td>-1.432662</td>\n",
              "      <td>0.485307</td>\n",
              "      <td>-0.141017</td>\n",
              "      <td>0.006652</td>\n",
              "      <td>1.460644</td>\n",
              "      <td>1.554225</td>\n",
              "      <td>-0.502457</td>\n",
              "      <td>0.963052</td>\n",
              "      <td>-0.411779</td>\n",
              "      <td>0.243155</td>\n",
              "      <td>-0.228313</td>\n",
              "      <td>0.182168</td>\n",
              "      <td>-1.408349</td>\n",
              "      <td>0.756096</td>\n",
              "      <td>-0.734525</td>\n",
              "      <td>-2.289946</td>\n",
              "      <td>-0.583719</td>\n",
              "      <td>0.909295</td>\n",
              "      <td>-0.786241</td>\n",
              "      <td>0.787203</td>\n",
              "      <td>-0.318500</td>\n",
              "      <td>3.739142</td>\n",
              "      <td>2.269577</td>\n",
              "      <td>0.049613</td>\n",
              "      <td>-0.262071</td>\n",
              "      <td>-0.689519</td>\n",
              "      <td>-2.047348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5226</th>\n",
              "      <td>-1.067559</td>\n",
              "      <td>-0.653070</td>\n",
              "      <td>-0.332214</td>\n",
              "      <td>-2.301044</td>\n",
              "      <td>-0.432093</td>\n",
              "      <td>-0.564634</td>\n",
              "      <td>-0.337867</td>\n",
              "      <td>0.011393</td>\n",
              "      <td>1.005828</td>\n",
              "      <td>0.418951</td>\n",
              "      <td>-0.417347</td>\n",
              "      <td>-0.398222</td>\n",
              "      <td>-0.629290</td>\n",
              "      <td>1.170893</td>\n",
              "      <td>-0.650829</td>\n",
              "      <td>-0.612817</td>\n",
              "      <td>0.494131</td>\n",
              "      <td>-0.771262</td>\n",
              "      <td>-1.605756</td>\n",
              "      <td>-1.764704</td>\n",
              "      <td>0.360886</td>\n",
              "      <td>1.513674</td>\n",
              "      <td>-0.638607</td>\n",
              "      <td>-2.525858</td>\n",
              "      <td>-0.182516</td>\n",
              "      <td>-1.057240</td>\n",
              "      <td>-0.493100</td>\n",
              "      <td>1.286383</td>\n",
              "      <td>0.214766</td>\n",
              "      <td>0.857357</td>\n",
              "      <td>2.497006</td>\n",
              "      <td>0.367569</td>\n",
              "      <td>0.134555</td>\n",
              "      <td>0.315001</td>\n",
              "      <td>-0.570234</td>\n",
              "      <td>-1.661436</td>\n",
              "      <td>-0.383391</td>\n",
              "      <td>0.515496</td>\n",
              "      <td>-2.382674</td>\n",
              "      <td>-0.091775</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.200995</td>\n",
              "      <td>-0.189984</td>\n",
              "      <td>-0.812358</td>\n",
              "      <td>1.382201</td>\n",
              "      <td>-0.533455</td>\n",
              "      <td>-0.075575</td>\n",
              "      <td>0.397567</td>\n",
              "      <td>-0.938185</td>\n",
              "      <td>-2.319425</td>\n",
              "      <td>-3.304437</td>\n",
              "      <td>0.547218</td>\n",
              "      <td>1.788698</td>\n",
              "      <td>-0.964402</td>\n",
              "      <td>-3.230963</td>\n",
              "      <td>0.285541</td>\n",
              "      <td>-0.847514</td>\n",
              "      <td>0.183561</td>\n",
              "      <td>1.592186</td>\n",
              "      <td>0.851443</td>\n",
              "      <td>0.722365</td>\n",
              "      <td>2.144270</td>\n",
              "      <td>0.952451</td>\n",
              "      <td>0.195022</td>\n",
              "      <td>-0.070279</td>\n",
              "      <td>-0.731923</td>\n",
              "      <td>-2.275405</td>\n",
              "      <td>0.393619</td>\n",
              "      <td>-0.152599</td>\n",
              "      <td>-3.726695</td>\n",
              "      <td>0.111620</td>\n",
              "      <td>2.356278</td>\n",
              "      <td>-1.038015</td>\n",
              "      <td>-0.063393</td>\n",
              "      <td>0.933021</td>\n",
              "      <td>4.690516</td>\n",
              "      <td>2.989251</td>\n",
              "      <td>1.311262</td>\n",
              "      <td>-0.740841</td>\n",
              "      <td>0.089524</td>\n",
              "      <td>-2.090379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5390</th>\n",
              "      <td>-1.259340</td>\n",
              "      <td>1.328986</td>\n",
              "      <td>0.596515</td>\n",
              "      <td>-2.928457</td>\n",
              "      <td>0.379648</td>\n",
              "      <td>0.257970</td>\n",
              "      <td>0.309743</td>\n",
              "      <td>-1.964013</td>\n",
              "      <td>0.911985</td>\n",
              "      <td>-1.011467</td>\n",
              "      <td>1.661287</td>\n",
              "      <td>1.039137</td>\n",
              "      <td>-0.269922</td>\n",
              "      <td>1.924874</td>\n",
              "      <td>-0.145452</td>\n",
              "      <td>0.355251</td>\n",
              "      <td>0.351788</td>\n",
              "      <td>-0.454910</td>\n",
              "      <td>-1.521293</td>\n",
              "      <td>-1.420369</td>\n",
              "      <td>0.674227</td>\n",
              "      <td>-0.680355</td>\n",
              "      <td>-2.086735</td>\n",
              "      <td>-1.800115</td>\n",
              "      <td>1.557943</td>\n",
              "      <td>0.081013</td>\n",
              "      <td>0.575901</td>\n",
              "      <td>1.627774</td>\n",
              "      <td>2.301168</td>\n",
              "      <td>-0.815836</td>\n",
              "      <td>0.323370</td>\n",
              "      <td>-0.763188</td>\n",
              "      <td>-0.339150</td>\n",
              "      <td>-0.300034</td>\n",
              "      <td>0.612166</td>\n",
              "      <td>-1.247223</td>\n",
              "      <td>0.393689</td>\n",
              "      <td>-1.046461</td>\n",
              "      <td>-2.670008</td>\n",
              "      <td>-0.218367</td>\n",
              "      <td>...</td>\n",
              "      <td>2.075116</td>\n",
              "      <td>1.451788</td>\n",
              "      <td>-0.431772</td>\n",
              "      <td>2.767171</td>\n",
              "      <td>0.091131</td>\n",
              "      <td>0.384108</td>\n",
              "      <td>0.290408</td>\n",
              "      <td>-0.307237</td>\n",
              "      <td>-1.998764</td>\n",
              "      <td>-2.224622</td>\n",
              "      <td>0.984401</td>\n",
              "      <td>-0.895574</td>\n",
              "      <td>-2.637071</td>\n",
              "      <td>-2.751663</td>\n",
              "      <td>2.710636</td>\n",
              "      <td>0.273257</td>\n",
              "      <td>1.481559</td>\n",
              "      <td>2.210135</td>\n",
              "      <td>3.757052</td>\n",
              "      <td>-1.248930</td>\n",
              "      <td>-0.312385</td>\n",
              "      <td>-1.268852</td>\n",
              "      <td>-0.839974</td>\n",
              "      <td>-0.589956</td>\n",
              "      <td>0.859093</td>\n",
              "      <td>-1.646516</td>\n",
              "      <td>0.629694</td>\n",
              "      <td>-1.761050</td>\n",
              "      <td>-4.440978</td>\n",
              "      <td>-0.191317</td>\n",
              "      <td>2.709678</td>\n",
              "      <td>-1.645555</td>\n",
              "      <td>1.877359</td>\n",
              "      <td>-0.696184</td>\n",
              "      <td>7.234523</td>\n",
              "      <td>2.774448</td>\n",
              "      <td>0.217255</td>\n",
              "      <td>-0.181247</td>\n",
              "      <td>-0.933627</td>\n",
              "      <td>-3.754542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>860</th>\n",
              "      <td>-0.932468</td>\n",
              "      <td>0.222813</td>\n",
              "      <td>0.021660</td>\n",
              "      <td>-1.702157</td>\n",
              "      <td>0.146959</td>\n",
              "      <td>0.070075</td>\n",
              "      <td>-0.012998</td>\n",
              "      <td>-0.825847</td>\n",
              "      <td>0.557764</td>\n",
              "      <td>-0.329856</td>\n",
              "      <td>0.315621</td>\n",
              "      <td>0.096680</td>\n",
              "      <td>-0.066470</td>\n",
              "      <td>0.964706</td>\n",
              "      <td>-0.286462</td>\n",
              "      <td>-0.157634</td>\n",
              "      <td>0.163019</td>\n",
              "      <td>-0.202315</td>\n",
              "      <td>-1.007095</td>\n",
              "      <td>-0.862622</td>\n",
              "      <td>0.535298</td>\n",
              "      <td>0.158265</td>\n",
              "      <td>-1.117840</td>\n",
              "      <td>-1.486827</td>\n",
              "      <td>0.741585</td>\n",
              "      <td>-0.223159</td>\n",
              "      <td>0.330194</td>\n",
              "      <td>0.917433</td>\n",
              "      <td>1.141281</td>\n",
              "      <td>0.004480</td>\n",
              "      <td>0.470831</td>\n",
              "      <td>-0.111404</td>\n",
              "      <td>-0.380747</td>\n",
              "      <td>-0.180239</td>\n",
              "      <td>0.325971</td>\n",
              "      <td>-0.835957</td>\n",
              "      <td>-0.077095</td>\n",
              "      <td>-0.364381</td>\n",
              "      <td>-1.415358</td>\n",
              "      <td>-0.104971</td>\n",
              "      <td>...</td>\n",
              "      <td>0.273809</td>\n",
              "      <td>-0.015898</td>\n",
              "      <td>-0.000278</td>\n",
              "      <td>1.085909</td>\n",
              "      <td>-0.290113</td>\n",
              "      <td>-0.068097</td>\n",
              "      <td>0.126860</td>\n",
              "      <td>-0.194955</td>\n",
              "      <td>-0.828866</td>\n",
              "      <td>-0.759069</td>\n",
              "      <td>0.604115</td>\n",
              "      <td>0.094512</td>\n",
              "      <td>-1.352372</td>\n",
              "      <td>-1.450465</td>\n",
              "      <td>0.927530</td>\n",
              "      <td>-0.133664</td>\n",
              "      <td>0.415313</td>\n",
              "      <td>0.916172</td>\n",
              "      <td>1.243397</td>\n",
              "      <td>-0.049298</td>\n",
              "      <td>0.350618</td>\n",
              "      <td>-0.129056</td>\n",
              "      <td>-0.470216</td>\n",
              "      <td>-0.191437</td>\n",
              "      <td>0.454349</td>\n",
              "      <td>-0.907637</td>\n",
              "      <td>-0.064774</td>\n",
              "      <td>-0.466105</td>\n",
              "      <td>-1.646682</td>\n",
              "      <td>-0.107840</td>\n",
              "      <td>1.518374</td>\n",
              "      <td>-0.564287</td>\n",
              "      <td>0.465769</td>\n",
              "      <td>-0.127536</td>\n",
              "      <td>2.766862</td>\n",
              "      <td>1.195344</td>\n",
              "      <td>0.554998</td>\n",
              "      <td>0.019136</td>\n",
              "      <td>-0.190313</td>\n",
              "      <td>-1.252681</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5391 rows × 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0         1         2  ...        97        98        99\n",
              "349  -1.144555 -0.180738 -0.268660  ... -0.152344  0.009660 -0.309653\n",
              "452  -1.102084  1.018204 -0.246298  ... -0.446956 -0.269786 -1.076639\n",
              "662  -1.147255 -0.089104  0.034505  ... -0.509304  0.039833 -1.384262\n",
              "172  -0.810159 -0.112330 -0.514646  ... -0.360762  0.422156 -1.071839\n",
              "1405 -1.390972  0.343716  0.071179  ... -0.376522 -0.228754 -1.782657\n",
              "...        ...       ...       ...  ...       ...       ...       ...\n",
              "3772 -0.826017  0.447423 -0.087454  ... -0.294015 -0.053580 -0.734272\n",
              "5191 -0.814944  0.310936 -0.501550  ... -0.262071 -0.689519 -2.047348\n",
              "5226 -1.067559 -0.653070 -0.332214  ... -0.740841  0.089524 -2.090379\n",
              "5390 -1.259340  1.328986  0.596515  ... -0.181247 -0.933627 -3.754542\n",
              "860  -0.932468  0.222813  0.021660  ...  0.019136 -0.190313 -1.252681\n",
              "\n",
              "[5391 rows x 100 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTidL42UXP8n",
        "colab_type": "code",
        "outputId": "62fb9c09-9f47-4955-9fe8-d4fc265900c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "cv_train_model(X_train.to_numpy(), y_train.to_numpy(), X_test.to_numpy(), y_test.to_numpy(), model=model1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-179-b6bb90b915a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-162-856934e40686>\u001b[0m in \u001b[0;36mcv_train_model\u001b[0;34m(X, y, X_test, y_test, folds, plot_feature_importance, model)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfold_n\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cross Validation mean score: {0:.4f}, std: {1:.4f}.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Score on test data: {0:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1097\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m                        \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                        zero_division=zero_division)\n\u001b[0m\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1224\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f-score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m                                                  \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1226\u001b[0;31m                                                  zero_division=zero_division)\n\u001b[0m\u001b[1;32m   1227\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0;32m-> 1484\u001b[0;31m                                     pos_label)\n\u001b[0m\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1299\u001b[0m                          str(average_options))\n\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1302\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'binary'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 90\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and continuous targets"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5h4cNokKZ_03",
        "colab_type": "code",
        "outputId": "7b88c919-2d74-489e-abfb-3caf54081670",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        }
      },
      "source": [
        "sklearn.model_selection.cross_val_score(model2, X, y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.42837274, 0.44506259, 0.46940195, 0.38734353, 0.37021573])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpve8lvranxb",
        "colab_type": "code",
        "outputId": "ac11d17c-8fcc-47d6-e787-cae4879906a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "source": [
        "sklearn.model_selection.cross_val_score(model3, X, y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.42837274, 0.42280946, 0.48191933, 0.38803894, 0.36812804])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYiBGD2CKZsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRU5623qST2i",
        "colab_type": "code",
        "outputId": "04745c28-c01d-4c23-e990-3747e4e66d06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "X_train.to_numpy()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3.4900000e+02, -1.1445554e+00, -1.8073829e-01, ...,\n",
              "        -1.5234390e-01,  9.6601040e-03, -3.0965292e-01],\n",
              "       [ 4.5200000e+02, -1.1020845e+00,  1.0182042e+00, ...,\n",
              "        -4.4695553e-01, -2.6978582e-01, -1.0766385e+00],\n",
              "       [ 6.6200000e+02, -1.1472551e+00, -8.9103766e-02, ...,\n",
              "        -5.0930446e-01,  3.9833285e-02, -1.3842623e+00],\n",
              "       ...,\n",
              "       [ 5.2260000e+03, -1.0675592e+00, -6.5306960e-01, ...,\n",
              "        -7.4084103e-01,  8.9524090e-02, -2.0903792e+00],\n",
              "       [ 5.3900000e+03, -1.2593403e+00,  1.3289862e+00, ...,\n",
              "        -1.8124712e-01, -9.3362670e-01, -3.7545419e+00],\n",
              "       [ 8.6000000e+02, -9.3246780e-01,  2.2281292e-01, ...,\n",
              "         1.9135714e-02, -1.9031273e-01, -1.2526808e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lx2A_jvBSTzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amb7_zR4JJh1",
        "colab_type": "code",
        "outputId": "b3a6fe5b-4c13-44ee-9a4e-611014f6738b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "source": [
        "cv_train_model(X_train.to_numpy(), y_train.to_numpy(), X_test.to_numpy(), y_test.to_numpy(), model=model1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AxisError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-126-b6bb90b915a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-115-6842a4f5c76a>\u001b[0m in \u001b[0;36mcv_train_model\u001b[0;34m(X, y, X_test, y_test, folds, plot_feature_importance, model)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0my_pred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0my_pred_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mvalid_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ovr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moof\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multi_class must be in ('ovo', 'ovr')\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         return _multiclass_roc_auc_score(y_true, y_score, labels,\n\u001b[0;32m--> 383\u001b[0;31m                                          multi_class, average, sample_weight)\n\u001b[0m\u001b[1;32m    384\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_multiclass_roc_auc_score\u001b[0;34m(y_true, y_score, labels, multi_class, average, sample_weight)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \"\"\"\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# validation of the input y_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         raise ValueError(\n\u001b[1;32m    442\u001b[0m             \u001b[0;34m\"Target scores need to be probabilities for multiclass \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     36\u001b[0m def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n\u001b[1;32m     37\u001b[0m          initial=_NoValue, where=True):\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m def _prod(a, axis=None, dtype=None, out=None, keepdims=False,\n",
            "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INEz8_QbMW8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##часть 2\n",
        "X = all_cos.drop('text_class', axis=1)\n",
        "y = all_cos.text_class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkHiWv77TIZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSTaBxClTcK4",
        "colab_type": "code",
        "outputId": "7c612e4e-41cb-4d3c-8452-71cee59d692c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        }
      },
      "source": [
        "cv_train_model(X_train.to_numpy(), y_train.to_numpy(), X_test.to_numpy(), y_test.to_numpy(), model=model1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AxisError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-131-b6bb90b915a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-115-6842a4f5c76a>\u001b[0m in \u001b[0;36mcv_train_model\u001b[0;34m(X, y, X_test, y_test, folds, plot_feature_importance, model)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0my_pred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0my_pred_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mvalid_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ovr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moof\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multi_class must be in ('ovo', 'ovr')\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         return _multiclass_roc_auc_score(y_true, y_score, labels,\n\u001b[0;32m--> 383\u001b[0;31m                                          multi_class, average, sample_weight)\n\u001b[0m\u001b[1;32m    384\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_multiclass_roc_auc_score\u001b[0;34m(y_true, y_score, labels, multi_class, average, sample_weight)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \"\"\"\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# validation of the input y_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         raise ValueError(\n\u001b[1;32m    442\u001b[0m             \u001b[0;34m\"Target scores need to be probabilities for multiclass \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     36\u001b[0m def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n\u001b[1;32m     37\u001b[0m          initial=_NoValue, where=True):\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m def _prod(a, axis=None, dtype=None, out=None, keepdims=False,\n",
            "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
          ]
        }
      ]
    }
  ]
}